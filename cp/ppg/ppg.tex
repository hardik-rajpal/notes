\documentclass{report}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage[a4paper,hmargin=0.8in,bottom=1.3in]{geometry}
\usepackage{lastpage,enumerate,fancyhdr,mathrsfs,xcolor,graphicx,listings,hyperref,enumitem}
\newcommand*{\algodis}[4]{
    \textbf{#1:} #2\\%name: % key idea
    \textbf{Time:} #3 \\% O(what)
    \textbf{Space:} #4
}
\author{Hardik Rajpal}

\begin{document}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}
\title{Pre-Placement Grind}
\maketitle
\tableofcontents
\pagebreak
\chapter{Week 1}
\section{Searching Algorithms}
\subsection*{Notes from \href{https://www.geeksforgeeks.org/searching-algorithms/}{GFG}}
These are algorithms to check for the existence of an element or to retrieve it from
a data structure. The retrieval can also involve only returning the position (index)
or a pointer to the element. There are two types:
\begin{enumerate}
    \item Sequential search: check every element based on a pre-determined sequence (ex. linear, alternating, etc.),
    and return the matches.
    \item Interval search: Designed for searching in \textbf{sorted} data structures.
    They involve \textbf{repeatedly} dividing the search space into intervals which
    can be excluded entirely after certain checks (ex. binary search).
\end{enumerate}
Some search algorithms are discussed below:
\begin{enumerate}
    \item \algodis{Linear Search}{Straighforward for-loop iterating over all elements in an array.}
    {O(n)}
    {O(1)}
    \item \algodis{Sentinel Linear Search}{Reduces the number of 
    comparisons by eliminating the need to check if the index is 
    within bounds. This is accomplished by appending the target
    element to the end of the array, and treating its index in the result as ``not found."}
    {O(n)}{O(1)}
    \item \algodis{Binary Search}
    {It's used for sorted arrays. It involves comparing the element
    at the center of the interval (defined initially as the entire array),
    with the target element. One of the halves of the interval is picked
    based on this comparison. The interval shrinks until the target is found
    or an interval of size one is not equal to the element. It can
    be implemented recursively or iteratively, each involving a step
    similar to $m = l + \frac{(r-l)}{2}$ while $l \leq r$}.
    {O(log(n))}
    {O(1)}
    \item \algodis{Meta Binary Search}
    {Seems unimportant but check it \href{https://www.geeksforgeeks.org/meta-binary-search-one-sided-binary-search/}{here}}
    {O(log(n))}{O(1)}
    \item \algodis{K-ary Search}
    {The search space is divided into k intervals in each step and one of them is picked to proceed further
    by comparing the target element to the interval markers.}
    {O(log(n)). The reduction is of a constant term: $log_k2$}
    {O(1)}
    \item \algodis{Jump Search}
    {The sorted array is examined in jumps of the
    optimal size $\sqrt{n}$,until the element being examined is greater than
    the target element. The interval is then shrunk to the previous interval.
    The shurnken interval can be examined linearly or with another jump search.
    }
    {O($2\sqrt{n} = O(\sqrt{n})$), or $O(n^{1/2} + n^{1/4} + n^{1/8}...) = O(\sqrt{n})$}
    {O(1)}
    \item \algodis{Interpolation Search}
    {It improves over binary search only if the data is uniformly 
    distributed. It involves selecting the splitting point of the 
    current search space by comparing the target value to the current lower and upper bounds of the space. Linear interpolation involves the following equations:\\
    $
    slope = (arr[r] - arr[l])/(r-l)
    $\\
    $
    m = l + slope \times (x - arr[l])
    $}
    {O(log(log(n))) on average, O(n) WCS.}
    {O(1)}
    \item \algodis{Exponential or Unbounded (Binary) Search}
    {We examine the search space from the lower end $l$,
    comparing $l+2^k - 1$ with the target element $x$, where $k$
    is the number of comparisons so far, until $x < arr[l+2^k - 1]$.
    Then, we examine the interval bounded by $l+2^{k-1} - 1$ and 
    $l+2^k - 1$, using binary search.}
    {O(log(n)), where n is the length of the array or where the 
    first occurrence of the target element exists in an unbounded 
    array.}
    {O(1)}
    \item \algodis{Fibonacci Search}
    {The array must be sorted. We first find the Fibonacci number $f(m)$ that exceeds the length of the given array. We compare the target element to the element at $arr[f(m-2)]$. We pick an interval based on the outcome.}
    {O(log(n))}
    {O(1)}
\end{enumerate}
\subsection*{Misc}
\begin{itemize}
    \item The preferred formula for evaluating the middle point of
    the interval in binary search is
    $m = l + (r-l)/2$, and not
    $m = (l+r)/2$, as the latter can suffer due to overflow.
    \item Global variables can also be used to maintain a ``best value yet" while searching through a space with binary search. For ex. find the first element $\geq$ x in an array.
    \item Problems where an array can be mapped to a boolean variable and is guarranteed to have either
    \begin{itemize}
        \item F...FT...T or
        \item T...TF...F
    \end{itemize}
    and our aim is to find the boundary between true and false
    values can be translated to a binary search problem, with 
    the target as the point where the variable changes:
    arr[i] != arr[i+1].
    \item Remember the \texttt{break} statement in iterative binary search if the middle point element is equal to the target.
    \item One can also binary search for a target range's starting point, instead of just a target. \href{https://leetcode.com/problems/find-k-closest-elements/}{See this problem.}
    \item In some cases, we might want to keep the current middle point \texttt{m} in the search space,
    here we resort to replacing either one of \texttt{r = m - 1} or \texttt{l = m + 1} by \texttt{ = m}
    and change the loop invariant \texttt{l <= r} to \texttt{l < r}. 
\end{itemize}
\section{Sorting Algorithms}
These algorithms rearrange a given array in ascending order.
Various other orders can be achieved by modifying the comparison operator.
A sorting algorithm is \textbf{stable} if it preserves the relative
order of equal elements.
\subsection*{Merge Sort}
The first part of the algorithm recursively handles halves of the given array.
The second part merges the halves sorted by the first part.
It takes O(nlog(n)) time in the \textbf{all cases}. O(n) space is necessary
for the merging side of affairs. Implemented recursively. It's advantages
include stability, parallelizability and lower time complexity. It's disadvantages
include higher space complexity and not being in-place, and that it's not
always optimal for small datasets. 
\subsection*{Quick Sort}
It involves recursively picking an element (\textbf{the pivot})
from the unsorted array, 
placing it so that all elements less than it are before and all
those greater than it are after. Then calling this function on the sub-arrays
after and before the chosen element.
//TODO pseudo code
\subsection*{Quick Sort}
TODO pseudo code
\subsection*{The Others}
\begin{enumerate}
    \item \algodis{Selection Sort}
    {The given array is viewed in two parts; sorted and unsorted.
    Every iteration involves \textbf{selecting} the minimal element
    and swapping it with the first element of the unsorted part. Hence,
    the boundary of the sorted part is expanded and that of the unsorted
    part has contracted. All of this happens inplace. It isn't stable.}
    {O($n^2$)}{O(1)}
    \item \algodis{Bubble Sort}
    {This involves repeatedly traversing the array,
    swapping any two \textbf{adjacent} elements if they are
    in the incorrect (descending) order, until we encounter
    a run with no swaps. It is stable. With each iteration,
    the last elements of the array are sorted in ascending order.}
    {O($n^2$)}
    {O(1)}
    \item \algodis{Insertion Sort}
    {It involves iterating over the array once, and in each iteration,
    if the current element is less than its left neighbour, we move it
    leftwards until its left neighbour is lower than it. It is in-place
    and stable.}
    {O($n^2$)}{O(1)}
    % \item \algodis{Radix/Counting Sort}
    % {}{}{}
\end{enumerate}
\chapter{Week 2}
TODO move notes over from the other notebook.
\chapter{Week 3}
\section{Complete Search}
\subsection*{Subset Processing}
We use the function below with 0. (n = size of given set.)
\begin{lstlisting}[language=C++,caption=Subset Generation]
void search(int k) {
    if (k == n) {
        // process subset
        subsets.push_back(subset);
    }
    else{
        search(k+1);
        subset.push_back(k);
        search(k+1);
        subset.pop_back();
    }
}
\end{lstlisting}
\begin{lstlisting}
for (int b = 0; b < (1<<n); b++) {
    //b runs from 00..00 to 11...11
    vector<int> subset;
    for (int i = 0; i < n; i++) {
        if (b&(1<<i)){
            subset.push_back(i)
        };
    }
}
\end{lstlisting}
\subsection*{Permutation Generation}
TODO: write up permutation ideas.
TODO: selection of subsets satisfying a property.
\subsection*{Backtracking En General}
If the dimensions of inputs are smaller than usual, backtracking is an option.
As with other algorithms, you want to maximize this as much as possible. Optimizations
are possible by:
\begin{enumerate}
    \item Transforming the inputs so as to reduce the search space.\\
    Example: If you are searching for a subset whose sum is a given target,
    Searching the space of frequency map is better than searching subsets in the
    untransformed set, at least when duplicates are abundant.
    \item Cutting off fruitless search paths as soon as possible. (Pruning the search tree.)
    \item Specifying "min" requirements before taking a path, and equivalently, specifying "max" allowed values in a path to be explored further.
    \item Optimizing the data structures used to record the current state and restrictions. Particularly,
    \begin{itemize}
        \item Using vectors instead of maps where possible.
        \item Using bitmap \texttt{int}s when only inclusion is to be checked.
    \end{itemize}
    \item Instead of using min/max to bring index values within range, which will likely incur repeated
    searches at the boundary, use an if block to disregard paths associated with values
    that exceed the bounds.
    \item A modification of the needle may speed up the search. For example, the search for a word
    may be sped up by searching for its reversed word if the end letter is less frequent than the letter at
    the start.
    
\end{enumerate}
The abstract code for backtracking looks like ths:
\begin{lstlisting}[language=C++]
    //declare global/class member variables.
    void search(int p){
        //p signifies path/position being inspected
        //in the search space.
        if(searchTerminalConditions()){
            if(globalVarSolutionValid){
                //update collection of solutions.
            }
        }
        else{
            for(possible path of exploration){
                //(1)update global vars so as to take this path.
                search(p+1);
                //(2)undo the updates made to global variables.
                //(not necessary if (1) overrides/uses previous updates.)
            }
            //undo any leftover changes made to global variables.
        }
    }
\end{lstlisting}
\textbf{Pruning:} A way of adding intelligence to the backtracking algorithm and
reducing the time spent in fruitless paths. Additionally, we can leverage symmetries
of the search space to check only a fraction of the entire possible solution set. Clearly,
optimizations at the start of the search tree save a lot more time than those at the end.
\subsection*{Meet in the Middle}
Another name for \textbf{Divide and Conquer}. It refers to splitting the search space up
into two halves and combining the results of the two halves. It works if there is an
efficient way to combine the results. Even 1 level of splitting (and extracting solutions
from the halves using brute force) can have worthwhile optimizations: O($2^n$) $\implies$ O($2^{n/2}$).

\chapter{Week 4}
\section{Greedy Algorithms}
\subsection*{\href{https://leetcode.com/discuss/general-discussion/1061059/ABCs-of-Greedy}{Reading Notes}}
Greedy Solutions focus on looking at the problem in smaller steps, and at each step
we select the option that offers the most obvious and immediate benefit. It's sort
of like assuming there's only one maximum point in the search space, and hence,
we just move in the direction with the most inclination. Some popular greedy 
algorithms are:
\begin{itemize}
    \item Dijkstra's shortest path.
    \item Kruskal's minimum spanning tree.
    \item Prim's minimum spanning tree.
    \item Huffman encoding.
\end{itemize}
With greedy algorithms, we often have to repeatedly pick
the minimal element from a collection; hence using a \texttt{priority\_queue}
or a \texttt{multiset} is often helpful.
\subsection{Union Find}
The data structure can also show up in greedy algorithms.
Given below is the most optimized implementation of \texttt{find} and \texttt{combine}.
\begin{lstlisting}[language=C++]
vector<T> items;//given vector of items.
vector<int> root;//representative roots of trees array.
vector<int> rank;//for combine optimization.
int find(int u){
    if(root[u]==u){return u;}
    //instead of return find(root[u]), do:
    root[u] = find(root[u]);//path compression
    return root[u];
}
void combine(int u, int v){
    int ru, rv;
    ru = find(u);rv = find(v);
    if(ru!=rv){
        //u, v in different trees.
        if(rank[ru] < rank[rv]){
            root[ru] = rv;
            rank[rv] += rank[ru];
            //combined tree has least possible height.
        }
        else{
            root[rv] = ru;
            rank[ru] += rank[rv];
        }
    }
}
\end{lstlisting}
\subsubsection*{Variations of \texttt{root} array}
The usual union-find implementation's root elements satisfy 
\texttt{root[r] == r}. However, we can also use negative numbers
at \texttt{root[r]} (which can't be the index of any parent),
and check for \texttt{root[r] < 0} when searching for the root. Such
a setup allows for recording information in the domain of negative
numbers at the root, say, the size of the tree, but negated. The
combine function then simply sets the combined tree's
root value to the confluence of values at \texttt{rv} and
\texttt{ru}.
\subsubsection*{Kruskal's MST Algorithm}
\begin{enumerate}
    \item Have a min-heap of all edges.
    \item Iterate through the heap, merging the trees of the vertices
    of each edge. For each non-trivial merge, update a counter. Additionally, add the edge to the list of edges for the MST or
    its weight to the weight of the MST.
    \item Once the merge counter is at $|V|$ - 1, break.
\end{enumerate}
\subsubsection*{Prim's MST Algorithm}
\begin{enumerate}
    \item Pick a starting vertex. Initialize an empty min-heap of edges. Maintain a count of visited vertices.
    \item Mark current vertex as visited.
    \item Add all edges going out of the current vertex to the heap.
    \item Iterate through the heap until an edge to an unvisited point is found.
    \item Set this point as the current point. Iterate until count of visited vertices = $|V|$.
\end{enumerate}
\subsubsection*{Dijkstra's Shortest Path}
\begin{enumerate}
    \item Pick a starting vertex. Maintain an array of minimum distances to reach any vertex from a visited vertex. For visited vertices, this should be -1.
    \item Update distances of array elements as min(old distance, distance from current point which is INT\_MAX if they are not neighbours). While iterating, record the array element with minimum distance to it. 
    \item Set the recorded element as the current vertex and continue until the current vertex is the target vertex.
\end{enumerate}
Modifications can be made to record the predecessors in the paths
or calculate the weights of the paths.
\begin{lstlisting}[caption=Shortest Path]
int distance(vector<int> &pi, vector<int> &pj);
int dijkstras(vector<vector<int>>& ps, int s, int target) {
    int n = ps.size(), res = 0, i = s;
    vector<int> min_d(n, INT_MAX);
    while (i != target) {
        min_d[i] = -1;
        int min_j = i;
        for (int j = 0; j < n; ++j){
            if (min_d[j] != -1) {//visited vertices.
                min_d[j] = min(min_d[j],distance(ps[i],ps[j]));
                min_j = min_d[j] < min_d[min_j] ? j : min_j;
            }
        }
        res += min_d[min_j];
        i = min_j;
    }
    return res;
}
\end{lstlisting}
\begin{lstlisting}[caption=Dijkstra's MST]
int distance(vector<int> &pi, vector<int> &pj);
int dijkstras(vector<vector<int>>& ps, int s, int target) {
    int n = ps.size(), res = 0, i = s,connected = 0;
    vector<int> min_d(n, INT_MAX);
    while (connected < n) {
        min_d[i] = -1;
        connected++;
        int min_j = i;
        for (int j = 0; j < n; ++j){
            if (min_d[j] != -1) {//visited vertices.
                min_d[j] = min(min_d[j],distance(ps[i],ps[j]));
                min_j = min_d[j] < min_d[min_j] ? j : min_j;
            }
        }
        res += min_d[min_j];
        i = min_j;
    }
    return res;
}
\end{lstlisting}
\subsubsection*{Stack Based Questions}
These usually involve finding the (lexicographically) minimal
subsequence. We maintain a stack to track the sequence
selected so far. To reverse a stack to get the subsequence, the
most optimal method is:
\begin{lstlisting}
while(s.size()){
    ans.push_back(s.top());
    s.pop();
}
reverse(ans.begin(),ans.end());
\end{lstlisting}
\subsubsection*{Heap+Queue}
Honestly I've only seen one question with this paradigm. However,
it's worth a shot if you realize you have to process numbers
starting always with the largest/smallest element, and have to 
track elements being available/unavailable over time.
I know that's a very vague and oddly specific situation,
but I couldn't just walk by a problem and not make this note.
\\
Additionally, in scheduling problems, consider trying to find
a way to arrange the given tasks, which might result in a
closed form solution.
\subsection{Greedy Matching}
Given two arrays to match elements such that the matching function
can be put into a total order over the elements (ISTG I will word this better,
later), we can sort two arrays and take the first matches offered
by traversing one array, selecting the first matched element with
the element being traversed.
\chapter{Aptitude Test Pointers}
\input{parts/aptitude}
\end{document} 