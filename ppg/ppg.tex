\documentclass{report}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage[a4paper,hmargin=0.8in,bottom=1.3in]{geometry}
\usepackage{lastpage,enumerate,fancyhdr,mathrsfs,xcolor,graphicx,listings,hyperref,enumitem}
\makeatletter
\def\maxwidth#1{\ifdim\Gin@nat@width>#1 #1\else\Gin@nat@width\fi}
\makeatother
\hbadness 100001
\newcommand{\mygraphic}[1]{
\begin{center}
    \includegraphics[width=\maxwidth{15cm}]{#1}
\end{center}
}
\newcommand*{\algodis}[4]{
    \textbf{#1:} #2\\%name: % key idea
    \textbf{Time:} #3 \\% O(what)
    \textbf{Space:} #4
}
\author{Hardik Rajpal}
\newcommand{\trow}[2]{\hline #1 & #2 \\}
\newcommand{\citem}[1]{\item \texttt{#1}}
\newcommand{\dsr}[5]{
\hline 
\texttt{#1} & #2 & #3 & #4 & #5\\
}
\newcommand{\specialcell}[2][c]{%
  \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}
\begin{document}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\setlist[enumerate,1]{label=\textbf{\arabic*.}}
\lstset{style=mystyle,language=C++}
\title{Pre-Placement Grind}
\maketitle
\tableofcontents
\pagebreak
\chapter{Misc.}
TODO: pragma O3 stuff
\section{Misc. Confirmed Optimizations}
\begin{enumerate}
    \item While using \texttt{cout}, \texttt{cerr} is fine during debugging, remember to remove them before submission as they greatly affect runtime.
    \item When writing custom comparators, avoid having memory accesses (\texttt{a[i][j]}) inside the comparator. Instead, modify the data structures whose elements are being compared to pass values that can be used in the comparator without memory accesses.
    \item For maps,
\begin{lstlisting}
    auto iter = m.find(k);
    if(iter!=m.end()){return iter->second;}
    //is much faster than:
    if(m.count(k)){return m[k];}
\end{lstlisting}
    \item Replace sets that check for inclusion by bit operations with an integer if the number elements $<$ 32 for ints and 64 for long longs. 
    \item Pass params by reference where possible.
    \item Instead of a reference parameter, consider using a pointer to that variable stored in a class data member.
    \item Replace data types by smaller data types where possible:
        \begin{itemize}
            \item long long by int
            \item int by char
        \end{itemize} 
    \item Replace fixed-length vectors by array$<$type,fixed-length$>$.
    \item Replace maps by vectors if they are indexed by integers within a fixed range. Ex: the alphabet as indices.
    \item That die roll problem optimization. (Needs to be phrased more mathematically.)
    \item Use of suffix arrays + descending order (or its prefix counterpart) to cut off paths in backtracking.
    \item \texttt{container.emplace} is better than \texttt{container.insert} but be wary of the syntax for containers of non-primitive objects. Emplace takes (a key and) arguments to their constructors while insert takes the (a pair of key and) object.
\end{enumerate}
\section{Binary Representations}
\subsection{Unsigned numbers}
Represent n using k bits $b_{k-1}b_{k-2}...b_0$:
\begin{equation}
n \in \{0,1,...2^{k}-1\}, n = \sum_{i=0}^{k-1}b_i\times2^{i}
\end{equation}
\subsection{1's complement}
Represent n using k bits $b_{k-1}b_{k-2}...b_0$:
\begin{equation}
n \in {-(2^{k-1}-1),...-0,0,1,...2^{k-1}-1}, n = (-1)^{b_{k-1}}(\sum_{i=0}^{k-2} (c_i) \times 2^{i})
\end{equation} 
\begin{itemize}
\item Where if $b_{k-1}==1$, $c_i=1-b_i$, else $c_i=b_i$.
\item It has two representations of zero: 1* and 0*.
\item MSB = 1 $\implies$ n $\leq$ 0, else n $\geq$ 0.
\item Bits to n is equivalent to 
\begin{itemize}
    \item Identify sign based on MSB.
    \item If MSB ==1, flip all bits.
    \item Find the unsigned integer they represent and combine the sign.
\end{itemize}
\item To take one's complement of a number is to find its negative counterpart.
\item It is equivalent to just flipping every bit.
\end{itemize}
\subsection{2's complement}
Represent n using k bits $b_{k-1}b_{k-2}...b_0$:
\begin{equation}
n \in \{2^{k-1}, 2^{k-1}-1, ... -1,0,1,...2^{k-1}-1\}, n = (-1)^{b_{k-1}}\sum_{i=0}^{i=k-1} b_i \times 2^i
\end{equation}
\begin{itemize}
\item To take 2's complement of a number is to flip its bits and add 1 to it.
\item This is equivalent to finding its negative in the 2's complement notation.
\end{itemize}
\subsection{Applications}
Using the fact that computers use 2's complement and bit operations, we can check certain facts about a number in O(1) time:
\begin{itemize}
\item $n = 2^k \iff (n>0) \&\& (n \& n-1 == 0)$
\item $n = 2^{2k} \iff (n>0) \&\& (n \& n-1 ==0) \&\& (n \& mask == n)$, where $mask = 0x55555555$.
\item Additionally, for any prime p, we can check if n is $p^k$, by checking if $ub \% n==0$, where ub is the largest power of p that fits within the integer upper bound in the language.
\end{itemize}
\section{Modular Arithmetic}
\begin{itemize}
\item a \% b returns the remainder when a is divided by b.
\item $a \% b = a - floor(a/b)*b$.
\item $a \equiv b mod n \iff a\%n == b\% n \iff (a-b) \% n == 0 $
\item The congruence relation modulo n splits integers into\\n residue (equivalence) classe $\{C_i\}_{i=0}^{i=n-1}$:
\begin{equation}
    C_i = \{z \| z \% n = i\};
\end{equation}
\item Representative of $C_i$ is $r_i \in C_i, 0 \le r_i \le n-1$
\item Operations:
\begin{itemize}
\item $a + b \% m = ((a \% m) + (b \% m)) \% m$
\item $a - b \% m = ((a \% m) - (b \% m)) \% m$
\item $(a*b) \% m = ((a \% m) * (b \% m)) \% m$
\item $(a/b) \% m = ((a \% m) * (b^{-1} \% m)) \% m$
\end{itemize}
\item Note: in c++,  a \% m where a$<$0 returns -((-a)\%m), which is not equal to the mathematical result of
(m - ((-a) \% m)). 
\item If a $<$ 0, a mod m should be calculated as (m + (a \% m)).
\item Or the following function extends to both negative and positive a's : (m + (a \% m)) \% m.
\item Exponentiation.
\item $a == b \mod m \implies a^e = b^e \mod m.$
\item Note: the converse is NOT true. It results in an algebraic equations with possibly multiple roots.
\item $a^e = a^c \mod n \iff e == c \mod \phi(n)$ and gcd(a,n) == 1.
\item If n is prime, then $a^e = a^{e \mod \phi(n)} \mod n \forall a$  
\item TODO: CRT, problems.
\end{itemize}

\section{Master Theorem for Recurrence}

\begin{equation}
T(n) \le aT(n/b) + cn^d \implies
T(n) \le a^{log_b(n)}T(0) + c.n^d.(\sum_{i=1}^{log_b(n)-1}{\frac{a}{b^{di}}})
\end{equation}
\begin{equation}
\implies T(n) = 
\begin{cases}
O(n^dlog(n)) & a = b^d \\
O(n^d) & a < b^d \\
O(n^{log_b(a)}) & a > b^d 
\end{cases}    
\end{equation}

\section{Useful Built-In C++ functions}
\begin{enumerate}
\item \texttt{swap()}
\item \texttt{lcm, gcd}
\item \texttt{cout<<std::boolalpha<<var\_boolean;} Prints true or false instead 1 or 0.
\end{enumerate}
\section{Design Rules}
\begin{enumerate}
\item One-Definition Rule: TODO
\end{enumerate}
\section{Watch out for these bugs}
\begin{itemize}
\item Integer overflow: if your algorithm returns a value absurdly low or higher than the set of possible values, check for integer overflow.
\begin{itemize}
    \item Change types of all non-negative integers to \texttt{unsigned long long}
    \item Types of all signed integers to \texttt{long long}.
\end{itemize}
\item When reusing a pointer to heap data, remember to set it to NULL after you \texttt{delete} it.
\item Prefer using (small.size()$>$large.size()+k) where k$\ge$0, as opposed to (small.size() - large.size() $>$ k). The two return values are unsigned and thus always yield a non-negative difference.
\item Separate out the iterating condition (limit, checks, increments to values) from a map before updating it, to avoid infinite loops and incorrect results.
\item \texttt{map.insert} doesn't override existing values at the given key. It is a no-op
if the key exists in the map already.
\end{itemize}
\section{STL}
\subsection{Sets and Maps}
These (\texttt{set} and \texttt{map}) are ordered data structures; helpful when
it is efficient to retain the ordering of a collection of elements during execution.
Their unordered counterparts (\texttt{unordered\_set} and \texttt{unorderd\_map})
prioritize access time and maintain no order of their elements.
\subsubsection*{Declaration syntaxes}
\begin{lstlisting}[caption={Sets and Maps},language=C++]
struct U{
    bool operator()(T t1, T t2)const{
        //logic comparing t1<t2.
    }
};
set<T,struct U> myset; multiset<T,struct U> mymulset;
map<T,V,struct U> mymap; multimap<T,struct U> mymulmap;
\end{lstlisting}
The ordering parameter \texttt{U} is crucial in cases where order between T is not inferable.
\begin{lstlisting}[caption={Unordered sets and maps},language=C++]
struct T{
    ...
    bool operator==(T t2)const{
        //return logic for t2==this.
    }
};
struct hashT{
    size_t operator()(T t)const{
        //std::hash<string or int or double>()(string or int or double)
        //return logic for hashing t. Use ^ << >> ~ | &
        //Note: Easy hash for collection of numbers=> sort, join with ","
    }
};
unordered_set<T,struct hashT> uset;
unordered_map<T,V,struct hashT> umap;
\end{lstlisting}
In the interest of speed and space, and unreadability of code, it might be
preferable to replace sets that only serve to mark and check membership
by flags and bit operations.\\
\begin{center}
    \fbox{\texttt{insert(index)}$\equiv$\texttt{flag|(1<<index)} and \texttt{count(index)}$\equiv$\texttt{flag\&(1<<index)}}
\end{center}
The data structure below outperforms \texttt{set}, \texttt{unordered\_set} and \texttt{bitset} in a program where only inclusion is to be checked, for a fixed range of integers.
\begin{lstlisting}
template<int N>
class Inclusion{
    int a[(N+31)/32];
public:
    Inclusion(){
        for(int i=0;i<(N+31)/32;i++){
            a[i] = 0;
        }
    }
    void insert(int i){
        a[i/32] |= 1 << (i % 32);
    }
    int count(int i){
        return a[i/32] & (1<<(i%32));
    }
};
\end{lstlisting}
\textbf{Note:} The great thing about sets and maps is the uniqueness of their values.
So, to find the maximum element less than an element, we simply \texttt{find()} the element
in the set and decrement the iterator.
\subsubsection{Erase}
\texttt{erase} can take the value to be erased or the iterator to it. With multisets
and multimaps, using the iterator ensures that the duplicates are not erased,
whereas using the value erases all duplicates. Note that the iterator can't be a
\texttt{reverse\_iterator}; we can't use \texttt{rbegin()} and must use
\texttt{end()} after decrementing it. 
\subsection{Priority Queue}
These structures lazily maintain order between their elements; only 
one extreme of the elements in the data structure is accessible at 
any time. The implementation involves a heap. the usage is as below:
\begin{lstlisting}[caption={Priority Queue},language=C++]
    struct U{
        bool operator()(T t1, T t2)const{
            //logic comparing t1<t2.
        }
    };
    priority_queue<T,vector<T>,struct U> pq;
\end{lstlisting}
The default U is \texttt{std::less<T>}. \texttt{pq.top()} returns
the largest element, based on the comparator.
\subsubsection{Getting the Kth value}
Some problems can ask for the Kth value (largest/smallest) from an
array of \texttt{val}s. These can be implemented
efficiently with PQs as follows:
\begin{lstlisting}[caption={Kth Largest Element},language=C++]
    priority_queue<int,vector<int>,greater<int>> pq;
    //greater=> pq.top == least value of pq.
    pq.push(val[0]);
    for(int i=1;i<val.size();i++){
        if(pq.size()<k){
            pq.push(val[i]);
        }
        else if(pq.top() < val[i]){
            pq.push(val[i]);
            pq.pop();
        }
    }
    return pq.top();
    //size of pq == k, and we have 
    //collected all high elements=>
    //pq.top == least value must be kth largest
\end{lstlisting}
\begin{lstlisting}[caption={Kth Smallest Element},language=C++]
    priority_queue<int,vector<int>,less<int>> pq;
    //less=> pq.top == max value of pq.
    pq.push(val[0]);
    for(int i=1;i<val.size();i++){
        if(pq.size()<k){
            pq.push(val[i]);
        }
        else if(pq.top() > val[i]){
            pq.push(val[i]);
            pq.pop();
        }
    }
    return pq.top();
    //size of pq == k, and we have 
    //collected all low elements=>
    //pq.top == max value must be kth smallest
\end{lstlisting}
The time complexity of these algorithms is
O((\texttt{val.size()})log(k)); it's linear in
\texttt{val.size()}.
An alternative way to maintain the kth element
in a list of elements, where deletion of specific
elements is necessary (but not permitted by the
priority\_queue data structure), we can use two sets.
\begin{lstlisting}[caption=Kth smallest Element,language=C++]
class Kset{
public:
    size_t k;
    multiset<int> trail;//multiset to permit duplicates.
    multiset<int> others;
    Kset(size_t _k){
        k = _k;
    };
    void insert(int e){
        if(trail.size()<k){
            trail.insert(e);
        }
        else{
            if(e<*trail.rbegin()){
                others.insert(*trail.rbegin());
                auto iter = trail.end();iter--;
                trail.erase(iter);
                trail.insert(e);
            }
            else{
                others.insert(e);
             }
        }
    }
    void erase(int e){
        auto iter = others.find(e);
        if(iter!=others.end()){
            others.erase(iter);
            //erase using iterators to avoid
            //erasing all duplicates
        }
        else{
            iter = trail.find(e);
            if(iter!=trail.end()){
                trail.erase(iter);
                trail.insert(*others.begin());
                others.erase(others.begin());
            }
        }
    }
    int top(){
        //returns kth smallest element.
        return *trail.rbegin();
    }
}
\end{lstlisting}
\subsubsection{Procedural PQ functions}
\begin{itemize}
\item A given vector \texttt{a} can be used as a heap in-place without transferring its contents anywhere using the functions below:
\begin{enumerate}
\item \texttt{make\_heap(first, last[, struct comp])}:moves elements in (first,last) container around to make a heap in O(N) time.
\item \texttt{push\_heap(first, last[, struct comp])}:inserts an element in the max heap \\represented by (first,last).
\item \texttt{pop\_heap}:removes the maximum element in the heap formed by (first,last).
\end{enumerate}
\end{itemize}
\subsection{Deque}
This is the most generic form of a container, combining methods for a stack, a queue and a vector (O(1) index accesses) allowing for:
\begin{enumerate}
\item \texttt{push\_front(val)}
\item \texttt{pop\_front()}
\item \texttt{push\_back(val)}
\item \texttt{pop\_back()}
\item \texttt{front()}
\item \texttt{back()}
\item \texttt{d[i]}
\item \texttt{insert(iter,val)}
\end{enumerate}
\subsection{The Big Fat Table of Data Structures}
\input{parts/bigfattable}
\subsection{Comparators}
STL provides its own simple comparators: (\texttt{T} can be replaced by
int, vector, etc.)
\begin{itemize}
    \item \texttt{less<T>} for ascending orders.
    \item \texttt{greater<T>} for descending orders.
\end{itemize} Custom comparators are written like so:
\begin{lstlisting}[caption={Comparators},language=C++]
struct U{
    bool operator()(const T& t1,const T& t2)const{
        //logic comparing t1<t2.
    }
};
\end{lstlisting}
Comparators are used as:
\begin{lstlisting}
set<T,struct ComparatorForT> myset;//note: type passed.
sort(v.begin(),v.end(),ComparatorForT());//note: instance passed.
\end{lstlisting}
\subsubsection{\href{https://stackoverflow.com/a/46128321/14681493}{Better Comparators}}
\subsection*{Static Functions}
\begin{lstlisting}
class Solution{
public:
    static vector<int> fm;
    static bool compare(int i1, int i2){
        return fm[i1] < fm[i2] || i1 < i2;
    }
    void solver(){
        set<int,decltype(Solution::compare)*> myset(Solution::compare);
        //decltype return function type,
        //appending * makes it a pointer.
        //The function is passed as an argument to the constructor.
        //or in a sort function:
        sort(inds.begin(),inds.end(),&(Solution::compare));
    }
};
Solution::vector<int> fm = {};
\end{lstlisting}
\subsection{Iterators}
Without going into the non-trivial hierarchy of iterators, note that:
\begin{itemize}
    \item \texttt{iter++} is supported by all iterators.
    \item \texttt{iter += n} is supported by random-access iterators, available with vectors and deques (and maybe others?).
    \item \texttt{void advance(iter,n)} is supported by all iterators: use this instead.
    \item \texttt{iter next(iter,n)} and \texttt{iter prev(iter,n)} is supported by all iterators.
    \item \texttt{distance(iter\_before,iter\_after)} is the more general version of \texttt{iter\_after - iter\_before}.
\end{itemize}
An iterator pointing at an element is ``corrupted" on removing the element.
Hence, any useful data should be copied over from the iterator before removing the element.
\begin{lstlisting}[caption={Undefined behaviour},language=C++]
    lists.erase(*minit);//minit is corrupted
    lists.insert((*minit)->next);
\end{lstlisting}
\begin{lstlisting}[caption={Working code},language=C++]
    lists.insert((*minit)->next);//first use the data.
    lists.erase(*minit);//then erase.
\end{lstlisting}
\subsection{Built-In Utilities}
Note: \texttt{iter} denotes the iterator return type. \texttt{first} and \texttt{last} denote \texttt{.begin()} and \texttt{.end()} iterators.
\begin{itemize}
    \citem{void sort(first,last[, struct comp])} (O(nlogn))
    \citem{iter partition(first,last,UnaryPredicate)} (O(nlogn))
    \begin{itemize}
    \item Returns the iterator to the first element of the second group.
    \end{itemize}
    \citem{void reverse(first,last)} (O(n))
    \citem{void random\_shuffle(first,last)} (O(n))
    \citem{iter max\_element(first,last[,struct comp])} (O(n))
    \citem{iter min\_element(first,last[,struct comp])} (O(n))
    \citem{int|long long|etc accumulate(first,last,init\_val[, function\_to\_combine(int,T)])} (O(n))
    \citem{iter lower\_bound(first,last,value)}(O(logn))
    \begin{itemize}
        \item Returns \texttt{iter} to the smallest element $\geq$ \texttt{value}.
    \end{itemize}
    \citem{iter upper\_bound(first,last,value)} (O(logn))
    \begin{itemize}
        \item Returns \texttt{iter} to the smallest element $>$ \texttt{value}.
    \end{itemize}
    \citem{bool next\_permutation(first,last[, struct comp])} (O(n))
    \begin{itemize}
        \item Updates \texttt{(first,last)} to its next permutation of in ascending order.
        \item Sort \texttt{(first,last)} first to access all permutations.
        \item Use in a \texttt{do-while} loop to avoid missing first permutation.
        \item Returns true if there exists a permutation greater than the current one.
    \end{itemize}
    \citem{bool prev\_permutation(first, last[, struct comp])} (O(n))
    \citem{void nth\_element(first, iteratorToNthElem,last[, struct comp])} (O(sdt::distance(first,last)))
    \begin{itemize}
        \item Alters the array so that *iteratorToNthElem is the nth smallest element.
        \item Modify the comparator to get nth largest element or other variations.
        \item All elements before iteratorToNthElem are less than the nth smallest element.
        \item All elements after iteratorToNthElem are greater than or equal to it.
    \end{itemize}
    \citem{void insert(v1.end(),v2.begin(),v2.end());} (O(v2.size()))
    \begin{itemize}
        \item Appends whole of v2 to end of v1.
    \end{itemize}
\end{itemize}

\chapter{Week 1}
\section{Searching Algorithms}
\subsection*{Notes from \href{https://www.geeksforgeeks.org/searching-algorithms/}{GFG}}
These are algorithms to check for the existence of an element or to retrieve it from
a data structure. The retrieval can also involve only returning the position (index)
or a pointer to the element. There are two types:
\begin{enumerate}
    \item Sequential search: check every element based on a pre-determined sequence (ex. linear, alternating, etc.),
    and return the matches.
    \item Interval search: Designed for searching in \textbf{sorted} data structures.
    They involve \textbf{repeatedly} dividing the search space into intervals which
    can be excluded entirely after certain checks (ex. binary search).
\end{enumerate}
Some search algorithms are discussed below:
\begin{enumerate}
    \item \algodis{Linear Search}{Straighforward for-loop iterating over all elements in an array.}
    {O(n)}
    {O(1)}
    \item \algodis{Sentinel Linear Search}{Reduces the number of 
    comparisons by eliminating the need to check if the index is 
    within bounds. This is accomplished by appending the target
    element to the end of the array, and treating its index in the result as ``not found."}
    {O(n)}{O(1)}
    \item \algodis{Binary Search}
    {It's used for sorted arrays. It involves comparing the element
    at the center of the interval (defined initially as the entire array),
    with the target element. One of the halves of the interval is picked
    based on this comparison. The interval shrinks until the target is found
    or an interval of size one is not equal to the element. It can
    be implemented recursively or iteratively, each involving a step
    similar to $m = l + \frac{(r-l)}{2}$ while $l \leq r$}.
    {O(log(n))}
    {O(1)}
    \item \algodis{Meta Binary Search}
    {Seems unimportant but check it \href{https://www.geeksforgeeks.org/meta-binary-search-one-sided-binary-search/}{here}}
    {O(log(n))}{O(1)}
    \item \algodis{K-ary Search}
    {The search space is divided into k intervals in each step and one of them is picked to proceed further
    by comparing the target element to the interval markers.}
    {O(log(n)). The reduction is of a constant term: $log_k2$}
    {O(1)}
    \item \algodis{Jump Search}
    {The sorted array is examined in jumps of the
    optimal size $\sqrt{n}$,until the element being examined is greater than
    the target element. The interval is then shrunk to the previous interval.
    The shurnken interval can be examined linearly or with another jump search.
    }
    {O($2\sqrt{n} = O(\sqrt{n})$), or $O(n^{1/2} + n^{1/4} + n^{1/8}...) = O(\sqrt{n})$}
    {O(1)}
    \item \algodis{Interpolation Search}
    {It improves over binary search only if the data is uniformly 
    distributed. It involves selecting the splitting point of the 
    current search space by comparing the target value to the current lower and upper bounds of the space. Linear interpolation involves the following equations:\\
    $
    slope = (arr[r] - arr[l])/(r-l)
    $\\
    $
    m = l + slope \times (x - arr[l])
    $}
    {O(log(log(n))) on average, O(n) WCS.}
    {O(1)}
    \item \algodis{Exponential or Unbounded (Binary) Search}
    {We examine the search space from the lower end $l$,
    comparing $l+2^k - 1$ with the target element $x$, where $k$
    is the number of comparisons so far, until $x < arr[l+2^k - 1]$.
    Then, we examine the interval bounded by $l+2^{k-1} - 1$ and 
    $l+2^k - 1$, using binary search.}
    {O(log(n)), where n is the length of the array or where the 
    first occurrence of the target element exists in an unbounded 
    array.}
    {O(1)}
    \item \algodis{Fibonacci Search}
    {The array must be sorted. We first find the Fibonacci number $f(m)$ that exceeds the length of the given array. We compare the target element to the element at $arr[f(m-2)]$. We pick an interval based on the outcome.}
    {O(log(n))}
    {O(1)}
\end{enumerate}
\subsection*{Misc}
\begin{itemize}
    \item The preferred formula for evaluating the middle point of
    the interval in binary search is
    $m = l + (r-l)/2$, and not
    $m = (l+r)/2$, as the latter can suffer overflow.
    \item Global variables can also be used to maintain a ``best value yet" while searching through a space with binary search. For ex. find the first element $\geq$ x in an array.
    \item Problems where an array can be mapped to a boolean variable and is guarranteed to have either
    \begin{itemize}
        \item F...FT...T or
        \item T...TF...F
    \end{itemize}
    and our aim is to find the boundary between true and false
    values can be translated to a binary search problem, with 
    the target as the point where the variable changes:
    arr[i] != arr[i+1].
    \item Remember the \texttt{break} statement in iterative binary search if the middle point element is equal to the target.
    \item One can also binary search for a target range's starting point, instead of just a target. \href{https://leetcode.com/problems/find-k-closest-elements/}{See this problem.}
    \item In some cases, we might want to keep the current middle point \texttt{m} in the search space,
    here we resort to replacing either one of \texttt{r = m - 1} or \texttt{l = m + 1} by \texttt{ = m}
    and change the loop invariant \texttt{l <= r} to \texttt{l < r}. 
\end{itemize}
\section{Sorting Algorithms}
These algorithms rearrange a given array in ascending order.
Various other orders can be achieved by modifying the comparison operator.
A sorting algorithm is \textbf{stable} if it preserves the relative
order of equal elements.
\subsection*{Merge Sort}
The first part of the algorithm recursively handles halves of the given array.
The second part merges the halves sorted by the first part.
It takes O(nlog(n)) time in the \textbf{all cases}. O(n) space is necessary
for the merging side of affairs. Implemented recursively. It's advantages
include stability, parallelizability and lower time complexity. It's disadvantages
include higher space complexity and not being in-place, and that it's not
always optimal for small datasets. 
\subsection*{Quick Sort}
It involves recursively picking an element (\textbf{the pivot})
from the unsorted array, 
placing it so that all elements less than it are before and all
those greater than it are after. Then calling this function on the sub-arrays
after and before the chosen element. See section at the end.

\subsection*{The Others}
\begin{enumerate}
    \item \algodis{Selection Sort}
    {The given array is viewed in two parts; sorted and unsorted.
    Every iteration involves \textbf{selecting} the minimal element
    and swapping it with the first element of the unsorted part. Hence,
    the boundary of the sorted part is expanded and that of the unsorted
    part has contracted. All of this happens inplace. It isn't stable.}
    {O($n^2$)}{O(1)}
    \item \algodis{Bubble Sort}
    {This involves repeatedly traversing the array,
    swapping any two \textbf{adjacent} elements if they are
    in the incorrect (descending) order, until we encounter
    a run with no swaps. It is stable. With each iteration,
    the last elements of the array are sorted in ascending order.}
    {O($n^2$)}
    {O(1)}
    \item \algodis{Insertion Sort}
    {It involves iterating over the array once, and in each iteration,
    if the current element is less than its left neighbour, we move it
    leftwards until its left neighbour is lower than it. It is in-place
    and stable. Best case happens when the array is sorted: O(n). Worst case
    is when it's in descending order: O($n^2$). Average time is O($n^2$).}
    {O($n^2$)}{O(1)}
    % \item \algodis{Radix/Counting Sort}
    % {}{}{}
\end{enumerate}
\section{Quick Sort}
\begin{itemize}
\item Involves two procedures: Partition and Quicksort
\item Partion takes an array, picks a pivot element in it, and rearranges the elements such that:
\begin{itemize}
    \item A prefix of the array contains all elements less than (or equal to) the pivot.
    \item A suffix of the array contains all elements greater than (or equal to) the pivot.
\end{itemize}
\item It returns the first index of a valid suffix where all values are $\ge$ the chosen pivot.
\begin{lstlisting}
int partition(vector<int> &a, int s, int e){
    //s,e inclusive.
    int pivot = a[pivotCriteria()];
    int i = s-1, j = e+1;//outer points to allow do-while
    while(i<j){
        do{j--;}
        while(j>s-1 && a[j]>=pivot);//note the equality.
        do{i++;}
        while(i<e+1 && a[i]<=pivot);//note the equality.
        if(i<j){
            swap(a[i],a[j]);
        }
    }
    return i;//first index of a valid suffix.
}
\end{lstlisting}
\item Quicksort partitions the array into two arrays, and makes recursive calls on each partition.
\begin{lstlisting}
void quicksort(vector<int> &a, int s, int e){
    if(e<=s){return;}
    int q = partition(a,s,e);
    //first index of suffix.
    quicksort(a,s,q-1);//sort the prefix
    quicksort(a,q,e);//sort the suffix.
}
\end{lstlisting}
\end{itemize}
\subsection{Quick Select}
\begin{itemize}
\item The partition function splits the array into two parts whose elements have an upper (for prefix) and lower (for suffix) bounds on their ranks in the sorted array.
\item This allows us to use it to find the kth (smallest) element in an array. The following algorithm is called quick select:
\begin{lstlisting}
vector<int> a;
int partition(int s, int e){
    int pivot = a[e];
    int i,j;
    i = s-1;
    j = e;
    while(i<j){
        do{j--;}
        while(j>s-1 && a[j]>=pivot);
        do{i++;}
        while(i<e && a[i]<=pivot);
        if(i<j){
            swap(a[i],a[j]);
        }
    }
    j++;
    swap(a[e],a[j]);
    return j;
    //return the start of the suffix, ensuring nums[suffixStart] = pivot.
}
int qselect(int s, int e, int k){
    int q;
    q = partition(s,e);
    if(q<k){
        return qselect(q+1,e,k);
    }
    else if(q>k){
        return qselect(s,q-1,k);
    }
    else{
        return a[q];
    }
}
\end{lstlisting}
\end{itemize}

\chapter{Week 2}
Topics: Stacks, Queues, linkedlists.
\section{Stacks}
\begin{enumerate}
\item Stacks can be used to maintain ``first element to the right $>$ '' for a given array.
\item Stacks can be used to select lexicographically least strings satisfying some property greedily.
\end{enumerate}
\section{Queues}
\begin{enumerate}
\item Use queues for level-order traversal of trees.
\begin{lstlisting}
while(q.size()){
    int n = q.size();
    while(n--){
        Node n = q.front(); q.pop();
        for(Node u:n.children()){
            q.push(u)
        }
        //Now q holds next level nodes of the tree.
    }
}
\end{lstlisting}
\end{enumerate}
\section{Linked Lists}
The following tasks can be done in O(1) space with singly linked lists:
\begin{enumerate}
    \item Finding a[n/2 - 1].
    \item Finding its length.
    \item Reversing the list.
    \item Floyd's cycle detection.
\end{enumerate}
\subsection{Floyd's Cycle Detection}
\begin{itemize}
\item Suppose the kth node in the list is the first node that's a part of the cycle which has n nodes.
\begin{center}
\includegraphics[width=8cm]{rsrc/floyd.jpg}
\end{center}
\item Let $p_s$ and $p_f$ represent the positions of the slow and fast pointers respectively.
\item Initially, $p_s = 0, p_f = 0.$
\item At $p_s = k$, $p_f = 2k$.
\item $\implies c = (2k-k) \% n = k \% n$ is the offset of the fast pointer when the slow pointer reaches the start of the cycle.
\item When, after m iterations, $p_s = k+m, p_f = 2(k+m)$ and $(p_s-k) == (p_f-k) \% n$.
\item $\implies (m + k) \% n = 0 \implies $ their current offsets in the cycle are (n-k)\% n.
\item $\implies $ If $p_a$ (new pointer) is $=0$, with k more iterations, the slow pointer's would be at $n-k + k = n$ (start of the cycle), as will $p_a$.
\end{itemize}

\chapter{Week 3}
\section{Complete Search}
\subsection*{Subset Processing}
We use the function below with 0. (n = size of given set.)
\begin{lstlisting}[language=C++,caption=Subset Generation]
void search(int k) {
    if (k == n) {
        // process subset
        subsets.push_back(subset);
    }
    else{
        search(k+1);
        subset.push_back(k);
        search(k+1);
        subset.pop_back();
    }
}
\end{lstlisting}
\begin{lstlisting}[language=C++]
for (int b = 0; b < (1<<n); b++) {
    //b runs from 00..00 to 11...11
    vector<int> subset;
    for (int i = 0; i < n; i++) {
        if (b&(1<<i)){
            subset.push_back(i)
        };
    }
}
\end{lstlisting}
\subsection*{Permutation Generation}
Permutation of a vector can be generated as follows:
\begin{lstlisting}
vector<int> permutation;
for (int i = 0; i < n; i++) {
    permutation.push_back(i);
}
do {
    // process permutation
} while (next_permutation(permutation.begin(),permutation.end()));
\end{lstlisting}
\subsection*{Backtracking En General}
If the dimensions of inputs are smaller than usual, backtracking is an option.
As with other algorithms, you want to optimize this as much as possible. Optimizations
are possible by:
\begin{enumerate}
    \item Transforming the inputs so as to reduce the search space.\\
    Example: If you are searching for a subset whose sum is a given target,
    Searching the space of frequency map is better than searching subsets in the
    untransformed set, at least when duplicates are abundant.
    \item Cutting off fruitless search paths as soon as possible. (Pruning the search tree.)
    \item Specifying "min" requirements before taking a path, and equivalently, specifying "max" allowed values in a path to be explored further.
    \item Optimizing the data structures used to record the current state and restrictions. Particularly,
    \begin{itemize}
        \item Using vectors instead of maps where possible.
        \item Using bitmap \texttt{int}s when only inclusion is to be checked.
    \end{itemize}
    \item Instead of using min/max to bring index values within range, which will likely incur repeated
    searches at the boundary, use an if block to disregard paths associated with values
    that exceed the bounds.
    \item A modification of the needle may speed up the search. For example, the search for a word
    may be sped up by searching for its reversed word if the end letter is less frequent than the letter at
    the start.
    
\end{enumerate}
The abstract code for backtracking looks like ths:
\begin{lstlisting}[language=C++]
    //declare global/class member variables.
    void search(int p){
        //p signifies path/position being inspected
        //in the search space.
        if(checkTerminalConditions()){
            if(globalVarSolutionValid){
                //update collection of solutions.
            }
        }
        else{
            for(possible path of exploration){
                //(1)update global vars so as to take this path.
                search(p+1);
                //(2)undo the updates made to global variables.
                //(not necessary if (1) overrides/uses previous updates.)
            }
            //undo any leftover changes made to global variables.
        }
    }
\end{lstlisting}
\textbf{Pruning:} A way of adding intelligence to the backtracking algorithm and
reducing the time spent in fruitless paths. Additionally, we can leverage symmetries
of the search space to check only a fraction of the entire possible solution set. Clearly,
optimizations at the start of the search tree save a lot more time than those at the end.
\subsection*{Meet in the Middle}
Another name for \textbf{Divide and Conquer}. It refers to splitting the search space up
into two halves and combining the results of the two halves. It works if there is an
efficient way to combine the results. Even 1 level of splitting (and extracting solutions
from the halves using brute force) can have worthwhile optimizations: O($2^n$) $\implies$ O($2^{n/2}$).

\chapter{Week 4}
\section{Greedy Algorithms}
\subsection*{\href{https://leetcode.com/discuss/general-discussion/1061059/ABCs-of-Greedy}{Reading Notes}}
Greedy Solutions focus on looking at the problem in smaller steps, and at each step
we select the option that offers the most obvious and immediate benefit. It's sort
of like assuming there's only one maximum point in the search space, and hence,
we just move in the direction with the most inclination. Some popular greedy 
algorithms are:
\begin{itemize}
    \item Dijkstra's shortest path.
    \item Kruskal's minimum spanning tree.
    \item Prim's minimum spanning tree.
    \item Huffman encoding.
\end{itemize}
With greedy algorithms, we often have to repeatedly pick
the minimal element from a collection; hence using a \texttt{priority\_queue}
or a \texttt{multiset} is often helpful.
\subsection{Union Find}
The data structure can also show up in greedy algorithms.
Given below is the most optimized implementation of \texttt{find} and \texttt{combine}.
\begin{lstlisting}[language=C++]
vector<T> items;//given vector of items.
vector<int> root;//representative roots of trees array.
vector<int> rank;//for combine optimization.
int find(int u){
    if(root[u]==u){return u;}
    //instead of return find(root[u]), do:
    root[u] = find(root[u]);//path compression
    return root[u];
}
void combine(int u, int v){
    int ru, rv;
    ru = find(u);rv = find(v);
    if(ru!=rv){
        //u, v in different trees.
        if(rank[ru] < rank[rv]){
            root[ru] = rv;
            rank[rv] += rank[ru];
            //combined tree has least possible height.
        }
        else{
            root[rv] = ru;
            rank[ru] += rank[rv];
        }
    }
}
\end{lstlisting}
\subsubsection*{Variations of \texttt{root} array}
The usual union-find implementation's root elements satisfy 
\texttt{root[r] == r}. However, we can also use negative numbers
at \texttt{root[r]} (which can't be the index of any parent),
and check for \texttt{root[r] < 0} when searching for the root. Such
a setup allows for recording information in the domain of negative
numbers at the root, say, the size of the tree, but negated. The
combine function then simply sets the combined tree's
root value to the confluence of values at \texttt{rv} and
\texttt{ru}.
\subsubsection*{Kruskal's MST Algorithm}
\begin{enumerate}
    \item Have a min-heap of all edges.
    \item Iterate through the heap, merging the trees of the vertices
    of each edge. For each non-trivial merge, update a counter. Additionally, add the edge to the list of edges for the MST or
    its weight to the weight of the MST.
    \item Once the merge counter is at $|V|$ - 1, break.
\end{enumerate}
\subsubsection*{Prim's MST Algorithm}
\begin{enumerate}
    \item Pick a starting vertex. Initialize an empty min-heap of edges. Maintain a count of visited vertices.
    \item Mark current vertex as visited.
    \item Add all edges going out of the current vertex to the heap.
    \item Iterate through the heap until an edge to an unvisited point is found.
    \item Set this point as the current point. Iterate until count of visited vertices = $|V|$.
\end{enumerate}
\subsubsection*{Dijkstra's Shortest Path}
\begin{enumerate}
    \item Pick a starting vertex. Maintain an array of minimum distances to reach any vertex from a visited vertex. For visited vertices, this should be -1.
    \item Update distances of array elements as min(old distance, distance from current point which is INT\_MAX if they are not neighbours). While iterating, record the array element with minimum distance to it. 
    \item Set the recorded element as the current vertex and continue until the current vertex is the target vertex.
\end{enumerate}
Modifications can be made to record the predecessors in the paths
or calculate the weights of the paths.
\begin{lstlisting}[language=C++,caption=Shortest Path]
int distance(vector<int> &pi, vector<int> &pj);
int dijkstras(vector<vector<int>>& ps, int s, int target) {
    int n = ps.size(), res = 0, i = s;
    vector<int> min_d(n, INT_MAX);
    while (i != target) {
        min_d[i] = -1;
        int min_j = i;
        for (int j = 0; j < n; ++j){
            if (min_d[j] != -1) {//visited vertices.
                min_d[j] = min(min_d[j],distance(ps[i],ps[j]));
                min_j = min_d[j] < min_d[min_j] ? j : min_j;
            }
        }
        res += min_d[min_j];
        i = min_j;
    }
    return res;
}
\end{lstlisting}
\begin{lstlisting}[language=C++,caption=Dijkstra's MST]
int distance(vector<int> &pi, vector<int> &pj);
int dijkstras(vector<vector<int>>& ps, int s, int target) {
    int n = ps.size(), res = 0, i = s,connected = 0;
    vector<int> min_d(n, INT_MAX);
    while (connected < n) {
        min_d[i] = -1;
        connected++;
        int min_j = i;
        for (int j = 0; j < n; ++j){
            if (min_d[j] != -1) {//visited vertices.
                min_d[j] = min(min_d[j],distance(ps[i],ps[j]));
                min_j = min_d[j] < min_d[min_j] ? j : min_j;
            }
        }
        res += min_d[min_j];
        i = min_j;
    }
    return res;
}
\end{lstlisting}
\begin{itemize}
\item Dijkstra's can be modified to count the number of minimum cost paths between two fixed points too.
\item This involves maintaining an array of \texttt{ways} denoting the number of min-cost ways to visit a vertex. The answer is \texttt{ways[dst]}. See \href{https://leetcode.com/problems/number-of-ways-to-arrive-at-destination/}{this problem.}
\item Dijkstra's algorithm can be used to solve the most generic weight problem:
\begin{itemize}
    \item Each edge u$\rightarrow$v has a weight $w(u,v)$;
    \item Each node has a weight $w_v(u)$;
\end{itemize}
\item We transform the problem to set each edge weight to $w'(u,v) = w(u,v) + w_v(v);$
\item We can use dijkstra's on this new graph so long as each edge weight is non-negative. Also note that the source node's weight is unaccounted for and must be added to the final answer.
\end{itemize}

\subsubsection*{Stack Based Questions}
These usually involve finding the (lexicographically) minimal
subsequence. We maintain a stack to track the sequence
selected so far. To reverse a stack to get the subsequence, the
most optimal method is:
\begin{lstlisting}[language=C++]
while(s.size()){
    ans.push_back(s.top());
    s.pop();
}
reverse(ans.begin(),ans.end());
\end{lstlisting}
\subsubsection*{Heap+Queue}
Honestly I've only seen one question with this paradigm. However,
it's worth a shot if you realize you have to process numbers
starting always with the largest/smallest element, and have to 
track elements being available/unavailable over time.
I know that's a very vague and oddly specific situation,
but I couldn't just walk by a problem and not make this note.
\\
Additionally, in scheduling problems, consider trying to find
a way to arrange the given tasks, which might result in a
closed form solution.
\subsection{Greedy Matching}
Given two arrays to match elements such that the matching function
can be put into a total order over the elements (ISTG I will word this better,
later), we can sort two arrays and take the first matches offered
by traversing one array, selecting the first matched element with
the element being traversed.
\subsection{Misc Data Structures}
Multiple problems tagged "Greedy" are really just a matter of
organizing the input data in a structure such as a (frequency)
map or a heap. Or we're just sorting the input array.
So, consider this when thinking of approaching 
a question greedily.
\section{2 pointers}
\begin{itemize}
\item Problems that apparently involve an O($n^2$) search space of solutions may be done in O(n), if a we can find some reason to ignore certain sets of coordinates.
\item With two-pointer approaches, we try to see if there's a reason to move any one particular pointer instead of the other.
\item An example problem would be \href{https://leetcode.com/problems/container-with-most-water/}{this} and \href{https://leetcode.com/problems/maximum-score-of-a-good-subarray/}{this}.
\end{itemize}
\chapter{Week 5}
\section{Dynamic Programming}
A common optimization to look out for when writing the code
for dynamic programming problems, try to ensure that
\begin{enumerate}
    \item The code doesn't compute paths that aren't going to be useful.
    \item The code doesn't recompute any path more than once.
\end{enumerate}
As per \href{https://leetcode.com/problems/house-robber/discuss/156523/From-good-to-great.-How-to-approach-most-of-DP-problems}{this article}, the approach to most dynamic programming problems can be broken
down to:
\begin{enumerate}
    \item Find recursive relation.
    \item Recursive (top-down).
    \item Add Memoization.
    \item Iterative + memoization (bottom-up).
    \item Further optimizations.
    \begin{itemize}
        \item Discarding paths
        \item Reducing space complexity.
    \end{itemize}
\end{enumerate}
\subsection{Common Patterns}
\subsubsection{Min (Max) Path to Reach Target}
\subsubsection{Distinct Ways}
\begin{itemize}
\item It might help to convert direction problems into terms involving sums and bounds, which are easier to reason about and enforce.
\end{itemize}
\subsubsection{Merging Intervals}
\subsubsection{DP on Strings}
\subsubsection{Decision Making}
\chapter{Graphs}
\section{Graph Algorithms}
\subsection{BFS|DFS}
I prefer writing both of these iteratively. In the immortal
intonation of Ashish Mishra,\\
\textbf{BFS} - \textbf{Queue}\\
\textbf{DFS} - \textbf{Stack}\\
Here's a sample of both algorithms.\\
\noindent\begin{minipage}{.45\textwidth}
    \begin{lstlisting}[caption=BFS,language=C++]
T s;
unordered_map<T,vector<T>> edges;
queue<T> q;
unordered_map<T,bool> visited;
unordered_map<T,T> prev;
int steps = 0;
q.push(s);
visited[s] = true;
while(!q.empty()){
    int sz = q.size();
    while(sz--){
        T u = q.front();
        q.pop();
        for(nb:edges[u]){
            if(!visited[nb]){
               visited[nb] = true;
               prev[nb] = u;
               q.push(nb);
            }
        }
    }
    steps++;
}
        \end{lstlisting}
\end{minipage}\hfill
\begin{minipage}{.45\textwidth}
    \begin{lstlisting}[caption=DFS,language=C++]
    T s;
    unordered_map<T,vector<T>> edges;
    stack<T> s;
    unordered_map<T,bool> visited;
    unordered_map<T,T> prev;
    s.push(s);
    visited[s] = true;
    while(!s.empty()){
        T u = s.top();
        s.pop();
        for(nb:edges[u]){
            if(!visited[nb]){
                visited[nb] = true;
                prev[nb] = u;
                q.push(nb);
            }
        }
    }
    \end{lstlisting}
\end{minipage}\\
The nodes should be reduced from the abstract type T, to
int whenever possible; thereby reducing the
\texttt{unordered\_map}s to \texttt{vector}s which can
sometimes get you under the time limit.
\href{https://stackoverflow.com/questions/55451825/why-is-vector-faster-than-unordered-map}{Click here for Why?}
\subsubsection*{Variations}
\begin{itemize}
    \item If the list of neighbours is a shared data structure, consider clearing it after having visited
    the neighbours using any one owner. Since, all elements in the shared field are visited and running them
    through the loop for other owners of the field is redundant. \href{https://leetcode.com/problems/jump-game-iv/}{(Leetcode)}
    \item Some algorithms may require the execution of a DFS to be identical to the recursive function call, with some processing at the parent after the children have been processed (searched at). This can be accomplished by: 
    \begin{itemize}
        \item using an adjacency list of queues (so as to \texttt{pop\_front} each child from the list after a visit)
        \item using an array for \texttt{childstart}, which maintains which index of the adjacency list we are to start inspecting the children.
    \end{itemize} 
    The latter method allows for reuse of the adjacency list whereas the former method destroys it.
\end{itemize}
\subsection{Topological Sort}
This can be implemented using BFS or DFS on a graph, starting from points with indegrees 0.
\begin{lstlisting}[caption=DFS implementation]
bool notDAG = false;
vector<int> vis(n,0);
vector<vector<int>> adj;//adjacency list.
vector<int> order;
void dfs(int u){
    if(notDAG){return;}
    vis[u] = 1;
    for(int v:adj[u]){
        if(vis[v]==1){
            notDAG = true;
            vis[u] = 2;
            return;
        }
        else if(vis[v]==0){
            dfs(v);
        }
    }
    vis[u] = 2;
}
//stack version:
vector<int> childstart(n,0);
void dfs(int u){
    stack<int> s;
    s.push(u);
    bool cproc = false;
    while(s.size()){
        cproc = false;
        u = s.top();
        vis[u] = 1;
        for(int v,i=childstart[u];i<adj[u].size();i++){
            v=adj[u][i];
            childstart[u]++;//to ensure this node isn't processed again as a child of u later.
            if(vis[v]==1){
                notDAG = true;
                return;
            }
            else if(vis[v]==0){
                s.push(v);
                cproc = true;
                break;
            }
        }
        if(!cproc){
            order.push_back(u);
            s.pop();
            vis[u] = 2;
        }
    }
}
\end{lstlisting}
\chapter{Misc}
\section{Misc. Algorithms}
\subsection{Binary Search}
While the idea of binary search is clear, opportunities
for its application may not be easily identified (yet). Some
common places where it may be applied:
\subsubsection*{Optimization Problems}
Problems involving the evaluation of the min/max of an expression,
while its constituents satisfy a constraint that is straightforward to
check. Consider the problem below:\\
\begin{center}
\fbox{Given $x_1,x_2,...x_n$ and $T$, find $min_{a_1,a_2,...a_n}(max_i(a_ix_i))$ such that $\sum_{i=0}^{n}{a_i}\geq T$ \href[]{https://leetcode.com/problems/minimum-time-to-complete-trips/description/}{(Leetcode)}}
\end{center}
In such problems, the answer involves
\begin{itemize}
    \item Drafting a binary search algorithm with \texttt{l, r} values chosen meaningfully.
    \item Drafting a \texttt{status(int m)} function that
    conveys which half of the search space we need to split.
\end{itemize}
The binary search algorithm is:
\begin{lstlisting}[language=C++,caption=BinSearch]
    int n = x.size();
    int mine = *min_element(x.begin(),x.end());
    int maxe = *max_element(x.begin(),x.end());
    unsigned long long lb = mine, ub = T*(unsigned long long)maxe;
    unsigned long long del = (ub - lb)/2;
    auto numtrips = [x,n](unsigned long long gt){
        unsigned long long nt = 0;
            for(int i=0;i<n;i++){
                nt += (gt/x[i]);
            }
            return nt;
    };
    while(del>0){
        if(numtrips(lb+del)>=T){
            ub = lb + del;
        }
        else{
            lb = lb + del;
        }
        del = (ub-lb)/2;
    }
    if(numtrips(lb)>=T){
        return lb;
    }
    return lb+1;
\end{lstlisting}
\subsection{Buckets}
Splitting a linear range up into buckets of size $\sqrt{n}$, and maintaining
necessary values (min, max, etc.) can help reduce time complexites from O(n)
to O($\sqrt{n}$).
\subsection{Split into subarrays}
The problem can be solved if the given array can be split into non-overlapping contiguous subarrays satisfying a given property.
TODO explain some more.
\subsection{Segment Tree}
Idk why this is so hyped man.
\begin{lstlisting}
class SegTree{
    vector<int> seg;//length of seg must be 4*n, where n is the array's length.
    int parent(int i){return i/2;}//similar to heaps.
    int left(int i){return 2*i+1;}
    int right(int i){return 2*i+2;}
    int combineSegs(int v1, int v2){
        return v1+v2;
        return max(v1,v2);
        return min(v1,v2);
    }
    int combId = 0 or INT_MIN or INT_MAX;//identity element of combineSegs
    void build(int i, int l, int r,vector<int> &a){
        if(l==r){
            seg[i] = a[l];
        }
        else{
            int del = (r-l)/2, li, ri;
            li = left(i);
            ri = right(i);
            build(li,l,l+del,a);
            build(ri,l+del+1,r,a);
            seg[i] = seg[li]+seg[ri];
        }
    }
    SegTree(vector<int> &a){
        int n = a.size();
        seg = vector<int>(4*n,0);
        build(0,0,n-1,a);
    }
    int query(int i, int s, int e, int l, int r){
        if(r<s || l>e){
            //l,r completely out of s,e
            return combId;//identity of merge.
        }
        else if(l<=s && r>=e){
            return seg[i];
        }
        else{
            int vl, vr, li, ri,del;
            li = left(i);
            ri = right(i);
            del = (e-s)/2;
            vl = query(li,s,s+del,l,r);
            vr = query(ri,s+del+1,e,l,r);
            return combineSegs(vl,vr);
        }
    }
    int findRange(int l, int r){
        return query(0,0,(seg.size()/4)-1,l,r);
    }
};
\end{lstlisting}
Alternatively, a \href{https://codeforces.com/blog/entry/18051}{less intuitive but efficient and short implementation} is:
\begin{lstlisting}
class SegmentTree{
    int n;
    vector<int> segs;
    void build(vector<int> &nums) {  // build the tree
        n = nums.size();
        segs = vector<int>(2*n,0);
        for(int i=0;i<n;i++){
            segs[n+i] = nums[i];
        }
        for (int i = n - 1; i > 0; --i){
            segs[i] = segs[i<<1] + segs[i<<1|1];
        }
    }

    void modify(int p, int value) {  // set value at position p
        p += n;
        segs[p] = value;
        for (; p > 1; p >>= 1){
            segs[p>>1] = segs[p] + segs[p^1];//parent val = combine current p and sibling of p.
        }
    }

    int query(int l, int r) {  // sum on interval [l, r)
        int res = 0;
        l+=n;
        r+=n;
        // if inclusive r, r+=1;
        for (; l < r; l >>= 1, r >>= 1) {
            if (l&1){ res += segs[l++];}
            if (r&1){ res += segs[--r];}
        }
        return res;
    }
};
\end{lstlisting} 
This is useful when:
\begin{itemize}
\item There are multiple queries asking about values computed over ranges.
\end{itemize}
\subsection{Array Scan}
The name is given to the family of algorithms where we do a couple of runs of a given array to evaluate an attribute. Approaches involving subarrays can be dealt with using to two indices \texttt{s} and \texttt{e}. Things to note:
\begin{itemize}
    \item Edge cases are possible at the start or end.
    \item The attribute evaluation will often be have to be done once more at the end of the loop.
    \item To improve performance, try to reduce the variables being updated/used in the loop.
    \item Two loops are useful for getting started with the code, but reducing them to one helps performance.
\end{itemize}
\subsection{Subarray Evaluations in Arrays}
\begin{itemize}
\item Some questions might require evaluating the min/max elements in subarrays. These can be pre-computed using
\textbf{prefix} and \textbf{suffix} arrays, or a \textbf{segment tree}.
\item  In a prefix array, \texttt{a[i]} denotes the min/max value
of the set \{\texttt{a[0],a[1]...a[i]}\} and in a suffix array, it denotes the min/max value
of the set \{\texttt{a[i],a[i+1],...a[n-1]}\}.
\item Additionally, \textbf{prefix and suffix sum} arrays
can be used to pre-compute cumulative sums for sub-arrays.
\item Another useful transformation is to generate consecutive prefix/suffix arrays, where the prefix is nullified 
at i if a[i]==0. See \href{https://leetcode.com/problems/maximal-rectangle/description/}{this problem.}
\item If a question involves finding
the max element less than or min element more than a value over a range (iter1, iter2), consider maintaining 
a sorted collection of the range, so as to use \texttt{lower\_bound} and \texttt{upper\_bound}, or custom \textbf{binary search functions}.
\item Suppose f is a cumulative operation defined over subarrays (say, bitwise-or, sum, or bitwise-xor), the computation can be inversed (subraction for addition, xor for xor) over the ranges, then \textbf{prefix/suffix arrays} can be used to compute
range queries (O(1)) instead of segment trees (O(logn)).
\item This works for xor and sum, but not for bitwise-or as it is not an invertible accumulation. 
\end{itemize}
\subsubsection{Two constraints}
Some problems, \href{https://leetcode.com/problems/closest-room/}{like this problem,} reduce to finding for each element \texttt{e} in \texttt{arr2}, an 
element in \texttt{arr1} whose \texttt{attr1} is greater than a value \texttt{f(e)} 
and \texttt{attr2} is minimized. These problems can be solved in min(n1log(n1),n2log(n2)) as follows:
\begin{lstlisting}[caption=Two constraints, language=C++]
    sort(arr2.begin(), arr2.end(), [](auto &a, auto &b) {
        return f(a) > f(b);
    });
    sort(arr1.begin(), arr1.end(), [](auto &a, auto &b){
        return a.attr1 > b.attr1;
    });
    int i = 0;
    set<int> attr2ValSet;
    vector<int> ans(arr2.size());
    for(auto e : arr2) {
        int minAttr1 = f(e);
        while(i < arr1.size() && arr1[i].attr1 >= minAttr1){
            attr2ValSet.insert(arr1[i].attr2);
            i++;
        }
        if(st.size()) {
            auto it = st.begin();
            //minimal attr2 that satisfies attr1 >= f(e).
            ans[e.idx] = *it;
        }
        else{
            ans[e.idx] = -1;
        }
    }
\end{lstlisting}
\subsubsection{Sub-Array Sum}
Here are some ideas I find useful in subarray sum questions:
\begin{enumerate}
    \item Compute the prefix|suffix sum arrays. Are they of any help?
    \item Divide and conquer: Try checking the condition for
    \begin{enumerate}
        \item a[0]...a[n/2 - 1] (recursively)
        \item a[n/2]...a[n - (n/2) - 1] (recursively)
        \item Compute prefix sum of (a) and suffix sum of (b)
        and check for subarrays formed by combining the two.
    \end{enumerate}
    If (c) can be done in less than O($n^2$) time, we've usually found a solution.
    \item Consider a sliding window along the array to capture selected
    subarrays. 
\end{enumerate}
\subsubsection{Spans}
Questions that involve finding the latest previous element that is greater than the 
current value in a sequence can be solved better by ignoring any values that are
surrounded by higher values; remove any element if it is smaller than its previous
element.\\
TODO: get infographic?


\subsection{Boyer-Moore Majority Voting}
\begin{itemize}
\item It returns the element occuring $>$ floor(n/2) times in an array of size n, in linear time.
\item A second pass can be used to ensure that count of the element returned is > floor(n/2), if its existence is not guarranteed.
\item In more complicated frequency questions, prefer going by the usual frequency map approach first.
\item Proof of correctness can be done inductively.
\begin{lstlisting}
vector<int> a;
int el, count=0;
for(int i=0;i<a.size();i++){
    if(count==0){
        el = a[i];
        count = 1;
        }
    else{
        if(a[i]==el){count++;}
        else{count--;}
    }
}
//Additionally to a second pass to ensure a majority element exists.
return el;
\end{lstlisting}
\item There also exists a version for finding elements with frequency > floor(n/k);
\begin{lstlisting}
vector<int> a;
int thres = n/k;
map<int,int> votes;
for(int i=0;i<a.size();i++){
    if(votes.count(a[i])){
        votes[a[i]]+=1;//use the optimized iterator version instead.
    }
    else{
        if(votes.size() < k-1){
            votes[a[i]] = 1;
        }
        else{
            bool used = false;
            for(auto entry:votes){
                if(entry.second==0){
                    votes.erase(entry.first);
                    votes[a[i]] = 1;
                    used = true;
                    break;
                }
            }
            if(!used){
                for(auto iter=votes.begin();iter!=votes.end();iter++){
                    iter->second -= 1;
                }
            }
        }
    }
}
for(auto entry:votes){
    entry.second = 0;
}
vector<int> ans;
for(int v:ans){
    if(entry.count(v)){
        entry[v] +=1;
    }
}
//Note that above loop takes O(n(logk))
//Last frequency check:
for(auto entry:votes){
    if(entry.second > thres){ans.push_back(entry.first);}
}
\end{lstlisting} 
\end{itemize}
\section*{Misc. Notes}
\subsection*{Precision printing}
\begin{lstlisting}
cout.setf(ios_base::fixed,ios_base::floatfield);
cout.precision(<d>);//to print d digits after dp.
cout<<a;
//or
printf("%.<d>f",a);
\end{lstlisting}

\chapter{OOP (in C++)}
\section{OOPs}
\begin{itemize}
    \item Access-specifiers:
    \begin{enumerate}
        \item private: can only be accessed inside the class.
        \item protected: can be accessed inside the class and inside derived classes.
        \item public: can be accessed everywhere.
    \end{enumerate}
\end{itemize}
\section{Inheritance from \href{https://www.tutorialspoint.com/cplusplus/cpp_inheritance.htm}{TutorialsPoint} and LearnCpp.com}
\begin{lstlisting}[language=C++,caption=Syntax]
    class DerivedClass: access-specifier BaseClass{
        //access-specifier is one on public/private/protected.
    };
\end{lstlisting}
\begin{itemize}
    \item Allows us to define a class in terms of another class.
    \item Derived classes inherit properties of base classes.
    \item Inheritance implements "is-a" relationship. Ex: mammal is-a animal, dog is-a mammal => dog is-a animal also holds.
    \item Derived classes inherit all properties of base classes except:
    \begin{enumerate}
        \item Constructors, destructors and copy constructors.
        \item Overloaded operators.
        \item Friend functions.
    \end{enumerate}
    \item The base classes can be inherited through public, 
    protected or private inheritance, which is specified by the 
    access specifier before its name in the declaration of the 
    derived class. The results are:
    \begin{enumerate}
        \item Public: access permissions of public and protected members of the base class are carried forward in the inherited class.
        \item Protected: access permissions of public and protected members of the base class are lowered to protected.
        \item Private: access permissions of public and protected members of the base class are lowered to private.
    \end{enumerate}
\end{itemize}
\subsection{Multiple Inheritance}
\begin{lstlisting}[language=C++,caption=Syntax]
    class DerivedClass: access-specifier baseA, access-specifier baseB ... {
    //access-specifier is one of public/protected/private.
    };
\end{lstlisting}
\begin{itemize}
\item \textbf{Mixins}: a small class that can be inherited from (in combination with other classes) to add properties to the derived class.
\item The constructors of parent base classes are called in the order that they are declared and before the constructor of the derived class.
\item Note that destructors are called in the completely reverse order of constructors. (Think of it as a stack of objects of base classes, with the derived class at the top).
\begin{lstlisting}[language=C++]
class Derived: public Base1, public Base2...
//base1 constructor.
//base2 constructor.
//derived constructor.
...
//derived destructor.
//base 2 destructor.
//base 1 destructor.
\end{lstlisting}
\item If two parent classes contain members with the same signature (name, args), a call to the signature from their common child class' object raises a compilation error. This is resolved using scope resolution operators.
\end{itemize}
\subsubsection{Diamond Problem of Multiple Inheritance}
\begin{itemize}
\item When two parent classes that share a base class are used to derive a child class, the inheritance tree looks like this:
\begin{center}
\includegraphics{rsrc/PoweredDevice2.png}
\end{center}
\item Each parent class has its own copy of the base class data members (resulting in redundant copies), and we can't call public members of the base class from the new derived class.
\item The diamond problem refers to our liking for a single instance of the base class in such cases of multiple inheritance, which is different from what happens when we try to implement such a hierarchy.
\item The solution is to use the keyword virtual while declaring the parent classes to identify them as virtual base classes:
\begin{lstlisting}[language=C++]
    class base{};
    class b1: virtual public base{};//
    class b2: virtual public base{};//without virtual in both of them, copies are made.
    class derived: public b1, public b2{};
\end{lstlisting}
\begin{itemize}
    \item Without \texttt{virtual}, each parent class maintains its copy of variables from the base class,\\
    and sizeof(derived) = (sizeof(b1)+size(b2)).
    \item With \texttt{virtual},\\ sizeof(derived) = (sizeof(b1 without b data)+sizeof(b2 without b data) + sizeof(b)+16B)
    \item The 16B are for book keeping. It can also be 8B on some systems.
    \item Note that all parent classes are prefixed with \texttt{virtual} and share ancestors, have a single copy of the ancestor's variables in the derived class.
    \item The book-keeping info grows with 8B for each new parent class. It doesn't grow with the size of the base class (or any class).
\end{itemize}
\item The construction of the base class becomes the responsibility of the derived class:
\begin{lstlisting}
class Copier: public Scanner, public Printer{
public:
Copier():PoweredDevice()/*base class*/, Scanner(), Printer(){
    //Order of constructor defintions run.
    //Base
    //base1
    //base2
    //Derived
}
}
\end{lstlisting}
\item The point above is true because of Printer, Scanner being virtual base classes. The order of constructors holds \textbf{even when single inheritance is done from a virtual base class}.

\item Accessing members of the root class, using an object of a class derived from two non-virtual sibling descendants of the root, leads to compilation errors. To avoid this, either declare the siblings as virtual descendants or use scope resolution operators (of the sibling classes, not the root!).
\end{itemize}
\section{Overloading}
TODO order of usage around operators study.
\begin{itemize}
    \item A single identifier (function name/operator) corresponds to two different implementations, based on the argument list supplied to it.
    \item Overload resolution refers to the compiler's task of selecting the most appropriate implementation when it encounters a call to an overloaded function.
    \item Note: operators can be overloaded outside classes too:
    \begin{lstlisting}[language=C++]
        //As a member function:
        Box operator+(const Box&);
        //Not as a member function:
        Box operator+(const Box&, const Box&);
    \end{lstlisting}
    \item In general, use const and \& for operands to 
    \begin{itemize}
        \item Avoid accidentally modifying them in the operation.
        \item Avoid time spent copying them around.
    \end{itemize}
    \item Most operators can be overloaded:
    \begin{center}
        \includegraphics[width=8cm]{rsrc/overloadableops.png}
    \end{center}
    \item Operators that can't be overloaded:
    \begin{enumerate}
        \item ::
        \item .*
        \item .
        \item ?:
    \end{enumerate}
    \item Unary operators include: ++ (post,pre), -- (post,pre), -, ~ and !
\end{itemize}
\subsection{Overloading ++ (and --)}
\begin{lstlisting}[language=C++]
class Digit{
//postfix: a++ : returns an rval.
Digit operator++(int){
    ... return digit;
}
//prefix: ++a : returns an lval. (original variable)
Digit& operator++(){
    ... return *this;
}
}
\end{lstlisting}
\begin{itemize}
\item Both definitions have something unique:
\begin{enumerate}
    \item \& in the prefix definition, to ensure it returns an lvalue.
    \item (int) in the postfix definition is necessary to distinguish it from the prefix definition, as c++ doesn't support return-value-based overloading.
\end{enumerate}
\item Also note that though operator++(int) looks like ++a (prefix), it's actually for a++ (postfix).
\end{itemize}
\section{Polymorphism}
Polymorphism means a call to a member function (after resolution of overloads), can lead to different implementations being called, based on the type of object that invokes the function.
\subsection{Static Linkage}
\begin{lstlisting}[language=C++]
class Shape {
   public:
      int area() {
         cout << "Parent class area :"...
      }
};
class Rectangle: public Shape {
   public:
      int area () { 
         cout << "Rectangle class area :"...
      }
};
class Triangle: public Shape {
   public:
      int area () { 
         cout << "Triangle class area :"... 
      }
};
int main() {
   Shape *shape;
   Rectangle rec(10,7);
   Triangle  tri(10,5);
   shape = &rec;
   shape->area();
   shape = &tri;   
   shape->area();
   //both calls print "Parent class area:..."
   return 0;
}
\end{lstlisting}
Without any prefixes in the functions defined in the derived classes (that are identifiable with functions in the base class),
the compiler assumes that any calls to these functions from an object of type base*/base always needs the implementation from the parent class. This is known as static linkage or static resolution (of the function call) or early binding, as the implementation for the area function is fixed at runtime to that of the base class (for objects of or pointers to the base class).\\
Note that calls from \texttt{rec} or \texttt{tri} would have called their respective functions, not the base class' function.
\subsection{Dynamic Linkage}
\begin{itemize}
\item Prefixing function identifiers that are shared across derived and base classes with \texttt{virtual}, allows for dynamic linkage, or dynamic resolution or late binding.
\item With the said prefix, the compiler identifies the right implementation to call by the contents of the object or pointer being used to invoke the function. This is polymorphism.
\item Note that a public function may be overridden by a private function, making the function inaccessible from objects of the derived classes.
\begin{lstlisting}
class Base{
    public:
    virtual int func(){
        cout<<"base"<<endl;
        return 0;
    }
};
class Derived: public Base{
    private:
    int func(){
        cout<<"derived, private"<<endl;
        return 1;
    }
};
int main(){
    Derived d;
    d.func();//compiler error: func is not accessible.
    Base &b = d;
    b.func();//compiles, prints "derived, private"=> allows accessing private function.
}
\end{lstlisting}
\end{itemize}
\subsubsection{Pure Virtual Functions}
If a function is always intended to be overridden in the derived classes and there's no meaningful definition in the base class,
we can just declare the function in the base class and set it to zero to avoid compilation errors about no definition being found for
the function in the base class.
\begin{lstlisting}[language=C++]
class Shape{
    //virtual int area();
    //Above line compiles if there are no to area() from any objects of shape/derived classes.
    //In the presence of such calls, even if area() is defined in derived classes and their
    //objects are used to call area() (from the right pointer, or a pointer of type Shape*)
    //compilation errors ensue.
    //However,
    virtual int area()=0;//goes through compilation successfully.
};
\end{lstlisting}
Note:
\begin{itemize}
\item We say a function demonstrates polymorphism if we can use a pointer of the base class
to access functions of different derived classes and have different implementations
being used based on the derived class to which the object belongs. 
\item Without the virtual keyword, even if functions with identical identifiers (name
and arguments considered) are declared in derived and base classes, polymorphism is not
observed. Function calls from pointers of the base class' type call its own implementation,
not that of derived classes. 
\item Once a function is declared as virtual in a class, it demonstrates polymorphism 
across all derived descendant classes, even without the virtual keyword being present 
intermediate classes.
\item The \texttt{final} keyword can be used to throw a compilation error if a function
that we don't want any base classes to override is overridden:
\begin{lstlisting}[language=C++]
class Rectangle:public Shape{
    public:
    int area()final{
        cout<<"Rect Area"<<endl;
        return 0;
    }
};
//Now if a class called square attempts to override area, a compilation error is thrown.
\end{lstlisting}
\item Using the final keyword in a non-virtual function (that was not declared to be virtual in any of the ancestors) throws a compilation error.
\item \textbf{Object Slicing:} When an object of a derived class is assigned to an object (not a pointer) of
a base class, only members inherited from the base class are kept and the others are discarded. Thus,
all functions that may have been overridden are reverted to their definitions in the base class.
\begin{lstlisting}[language=C++]
void printarea(Shape s){
    s.area();
}
void printareaReference(Shape &s){
    s.area();//NO slicing. 
}
Shape s; Rect r;
s = r;//slicing.
Shape &s = r;
printarea(r);//slicing. prints "Shape area..."
printareaReference(r);//No slicing. prints "Rectangle area..."
\end{lstlisting}
In JAVA, and other languages where each non-primitive variable is actually a reference, object
slicing doesn't happen.
\end{itemize}
\section{Data Abstraction and Encapsulation}
The idea is to write classes with a well-defined boundary between:
\begin{enumerate}
\item Implementation: how the class works, the variables and functions it needs for its work.
\item Interface: the function calls and variables accessible to the users of the class.
\end{enumerate}
Data abstraction allows:
\begin{itemize}
\item Implementation of a class to evolve without affecting code that uses it.
\item Prohibiting users from possibly disturbing the state of the objects of the class,
which may affect correctness of its functions. Ex. A user sets the \texttt{top} pointer
inside a stack's implementation to the start of the array, without updating the length,
which is non-zero. This results in a segmentation fault.
\end{itemize}
It is enforced using access specifiers. Data encapsulation is about
bundling all the related data and functions that use it into one class,
keeping as much implementation detail from the user as possible.
\section{Abstract Classes a.k.a C++ Interfaces}
An abstract class is a class with at least one \textbf{pure virtual function}.
Such classes define an interface that all derived classes have to support (have an
implementation of).
\section{Notes from interview questions}
\begin{itemize}
    \item In multilevel inheritance (A->B->C), any function calls from an object of type C are linearly searched for up the hierarchy, and the first implementation is taken.
    \item Pointers of a parent type can hold a child, but child pointers being assigned to parent objects raises compilation errors.
    \item Pointers of a parent type can only access members (variables and functions) declared and declared public in the parent.
    \item When a derived class defines a function with the same name as some function its base class, all functions (even with different signatures) of the base class with the same name become inaccessible to objects.
    \item However, using a pointer of the base type to point to the object of the derived class, both the overridden method and the unoverridden overload of a method with the same name can be accessed.
    \item Or, using a scope resolution operator:
    \begin{lstlisting}[language=C++]
        d.Base::fun(5);//goes through.
        Base &b = d;
        b.fun(5);//goes through.
    \end{lstlisting}
\end{itemize}
\subsubsection{Initializer Lists}
\begin{itemize}
\item Initializer lists of a derived class can't
include members of the base class. They need to be initialized using the contructor of the base class.
\end{itemize}
\subsection{Destructors}
\begin{itemize}
\item The default destructor of a base class is accessible to everyone.
\item If it is overridden, it must be specified at least as protected.
\item If the destructor is declared as protected, calls to \texttt{delete baseptr}
throw compilation errors.
\item Ideally, if \texttt{baseptr}s are intended to be used for polymorphism and 
we want to avoid \texttt{delete baseptr} calls, we should set the destructor to protected.
\item Finall, if a the destructor is public, and \texttt{delete baseptr} is called, the 
destructor of any derived class object held in baseptr is not called, and the chain of destructors
starts being called from that of the baseptr's class.
\end{itemize}
\subsection{New and Delete}
TODO
\subsection{Copy Constructors}
\begin{lstlisting}[language=C++]
class Sample{
    int id;
    Sample(Sample &t)
    {
        id=t.id;
    }
};
//defines what do to do when:
Sample a,b;
a = b;//calls copy constructor of a.
\end{lstlisting}
\begin{itemize}
\item Used to intialize members of a newly created object by copying members of an already existing object.
\item It takes a reference parameter of an object of the same class.
\item This is known as copy initialization, a.k.a. member-wise initialization.
\item If not defined explicitly by the programmer, the compiler 
defines it for us.
\item The following definition of a copy constructor makes the object of this class uncopyable:
\begin{lstlisting}
class Derived{
    Derived(const Derived&) = delete;
};
\end{lstlisting}
\item Attempting to copy \texttt{Derived} objects in the code throws a compilation error.
\item Hence, \texttt{throw d} also throws a compilation error, as \texttt{throw} implies copy operations. (See Exceptions section.)
\end{itemize}
\subsubsection{Types of Copy Constructors}
\begin{enumerate}
\item Default Copy Constructor: The implementation offered by the compiler which copies the bases and members of an object in the same order that a constructor would intialize the bases and members of the object.
\item User Defined Copy Constructor: needed when an object owns pointers or non-shareable references, such as to a file. A destructor and assignment operator should also ideally be written in this case to assist in transfer/destruction of said references.
\end{enumerate}
A copy constructor is called when:
\begin{itemize}
\item An object of the class is \textbf{returned by value.}
\item An object of the class is \textbf{passed by value} as an argument.
\item An object is constructed based on another object of the same class. Ex: \texttt{Shape s1 = {1,2}; Shape s2(s1) or Shape s2 = s1;}
\item The compiler generates \textbf{a temporary object.}
\end{itemize}
Note that it's not called when a previously declared object is assigned another object. This calls the assignment operator.
\begin{lstlisting}[language=C++]
Shape s1,s2;
s1 = {1,3};
Shape s3(s1);//calls copy constructor.
Shape s4 = s1;//calls copy constructor.
s2 = s1;//calls assignment operator.
\end{lstlisting}
Note that a call to the copy constructor is not guarranteed as
the compiler performs optimizations like \textbf{return value 
optimization} and \textbf{copy elision} to avoid unnecessary 
copies where possible. (TODO)\\
Other points:
\begin{itemize}
\item Use a user-defined copy constructor when the default copy constructor results in a shallow copy (say, if some members are pointers). Deep-copy is only possible in a user-defined constructor.
\item Copy constructors can be made private, and this makes objects of the class non-copyable. It's particularly useful (as a lazy technique to avoid shallow copies) if the class has pointers of dynamically allocated resources. The right way is to write a deep-copy-constructor and make it public.
\item A copy constructor which takes the object argument by value leads to a compilation error, as at runtime it would have lead to an infinite chain of copy constructor calls.
\item Use const in the argument to make sure:
\begin{enumerate}
    \item The source object isn't accidentally modified.
    \item The copy-constructor can be called with temporary objects created by the compiler, which can't be bound to non-const references.
    \begin{lstlisting}[language=C++]
        //if copy constructor doesn't say const Shape &s1,
        Shape s2 = fun();//fun returns s2 by value.
        //the above code throws a compilation error, at the last line.
        //If const is present, it compiles.
    \end{lstlisting}
\end{enumerate} 
\item In default constructors, default constructors of parents are called before those of derived classes, but, in copy constructors, the parent's default constructors (not copy constructors are called), unless the implementation of the derived class' copy constructor specifically calls their copy constructors.
\end{itemize}
\subsubsection{Copy Elision (a.k.a. Copy Omission)} 
The compiler avoids making copies of objects (in pass by value/return by value scenarios) where possible.

\section{From LearnCPP.com}
\subsection{14.4 Const objects}
\begin{lstlisting}[language=C++]
const Date today {2020, 10, 14};//valid.
const Date today = {2020, 10, 14};//valid.
//Note that const objects must be initialized, unless a default
//constructor is defined.
\end{lstlisting}

Objects that are declared with \texttt{const} keyword (as a local variable, or a function argument) impose certain restrictions (that upon violation lead to compiler errors.):
\begin{itemize}
\item Their members variables can't be changed, neither via direct access nor calls to member functions that change them.
\item Additionally, const objects can't call non-const member functions. \texttt{const} before the definition body indicates that the member function doesn't modify the members of the class; it doesn't impose any restrictions on the returned value or aruments.
\begin{lstlisting}[language=C++]
class Date{
    //can't be called by a const object, despite not
    //altering any variables in its definition:
    void print(){
        cout<<"non-const member function.";
    }
    //can be called by const objects:
    void print2() const {
        cout<<"const member function.";
    }
}
\end{lstlisting}
\item If the declaration and definition are written separately, \texttt{const} must be present after the function signature in both places.
\begin{lstlisting}[language=C++]
class Date{
    void print() const;
};
void Date::print()const{
    ...
}
\end{lstlisting}
\item An attempt to modify the class inside a const function raises a compilation error, even inside unreachable if-blocks.
\item Within the definition of a const member function, \texttt{this} is a const pointer to a const object.
\item No constructor can be declared as a constant, as they need to modify the member variables, regardless of what their implementation says.
\item It is perfectly fine to call const member functions from non-const objects.
\item Functions can be overloaded based on whether they are const or not. So, const objects call the const variant while non-const objects call the non-const variant. This is usually done if constness changes the return value.
\begin{lstlisting}[language=C++]
//These are valid overloads:
int fun(){

};
int fun()const{

};
//These are invalid as const keyword specifiers return type.
int fun(){}
const int fun(){}
//Also note that
const int fun(){};
//is exactly the same as:
int const fun(){};
//and the two are different from
int fun()const{};
\end{lstlisting}
\end{itemize}
\subsection{14.6 Access functions}
\begin{itemize}
\item Trivial member functions to access selectd private data members.
\item Of two types: Setters (mutators) and getters (accessors).
\item Getters are made const so they can be called on const objects while setters have to be non-const.
\item For efficiency, getters can be written to return constant lvalue references, instead of returning by value:
\begin{lstlisting}
const std::string &getName()const{return m_name;}
\end{lstlisting}
\item Note that such functions' returned references become invalid the moment the object is destroyed. So, the references should not be stored (and accessed) beyond the lifetime of the object.
\begin{lstlisting}
const std::string & ref = createEmployee().getName();
//we store the reference to a property in the rvalue implicit
//temporary object, created by the compiler.
//accessing ref later leads to undefined behaviour. 
\end{lstlisting}
\item References returned from functions to private members should be constant; otherwise, they permit direct modification of private members.
\end{itemize}
\subsubsection{Ref-qualifier overloads}
\begin{lstlisting}
const std::string& getName() const & { return m_name; } // when called on an lvalue (single &), return member by reference
const std::string getName() const && { return m_name; } // when called on an rvalue (double &&), return member by value
// Alternately, we can disable the use of a member functions for rvalue objects
const std::string& getName2() const && = delete;         // when called on an rvalue, emit a compilation error
\end{lstlisting}
\section{Friends}
A friend is a class or function (member or non-member) that has been granted full access to the private and protected members of another class. Using friends, classes can selectively give full access to their members without unnecessary impacts.\\
The friendship is established by (declared in) the class removing its access control for some other entity.
\subsection{Friend (non-member) Functions}
With non-member functions, an object of the class must be accepted as an argument to access the relevant data.
\begin{lstlisting}
class Shape{
    int area;
    friend double paintcost(const Shape& shape);
};
double paintcost(const Shape& shape){
    return shape.area*costunit;//accesses private member and compiles.
}
\end{lstlisting}
\begin{itemize}
\item Note that the friend function can also be defined inside the class, and remain a non-member function because of the \texttt{friend} keyword.
\item A function can be a friend of multiple classes, all of 
which appear in its argument list. These are used when it makes 
less syntactic sense to make the function a member of either 
class.
\begin{lstlisting}
friend void printWeather(const Temperature& temp, const Humidity& hum){
    //access private members of both temp and hum at once.
}
\end{lstlisting}
\item Access specifiers make no difference to the availability of friend functions as they are non-members anyways.
\item Friend functions should also use the class' interface where possible, instead of directly accessing data, as this insulates them from future change in the class.
\end{itemize}
\subsection{Friend Classes and Member Functions}
\begin{itemize}
    \item Friend classes can access private and protected members of another class.
\begin{lstlisting}
class Storage{
    //private members here.
    friend class Display;
    //Display declared as a friend of storage.
    //Display accesses all members of storage.
    //Note: no forward declaration required.
};
class Display{
    void print(const Storage& storage);
}
\end{lstlisting}
\item Friendship is not reciprocal.
\item Friendship is not transitive.
\item Frienship is not inherited. Classes derived from a friend are not friends.
\end{itemize}
\subsubsection{Friend Member Functions}
\begin{itemize}
\item One point worth prattling about is that the compiler needs to have seen the declaration of a member function before it can be declared as a friend somewhere using the syntax below:
\begin{lstlisting}
class Storage{
    //private members.
    friend void Display::displayStorage(Storage &storage);
    //compiler should have seen displayStorage in Display 
    //before this line.
};
\end{lstlisting} 
\item This implies the compiler should have encounted the \textbf{full definition of the class} to which the member function belongs, not just a forward declaration.
\item Additionally, to use members of the class where friendship is declared (\texttt{Storage}), it should have been declared before the definition of the friend member function.
\item A better solution is of course to split the code up into separate files.
\end{itemize}
\chapter{C Stuff}
\section{Calloc vs Malloc}
\begin{center}
\begin{tabular}{| p{8cm} | p{8cm} |}
    \trow{Malloc}{Calloc}
    \trow{\texttt{(void*) malloc(NumBytes)}}{\texttt{(void*) calloc(NumElems, ElemSizeInBytes)}}
    \trow{Stands for memory allocation.}{Stands for clear memory allocation.}
    \trow{Returned memory block has garbage values.}{Returned memory block is initialized to 0.}
    \trow{Faster than calloc.}{Slower than malloc.}
    \trow{Doesn't involve memory overheads.}{Involves some memory overheads.}
    \hline
\end{tabular}
\end{center}
\chapter{More C++ Stuff}
\section{Templates from LearnCpp.com}
\begin{itemize}
\item Templates are declarations of functions of classes with placeholder types.
\item Placeholder types aka, type template parameters, aka template types.
\item A placeholder type is not known at the definition of the template, but is decided at a call to the template.
\item Once a template is written, the compiler can use it to generate as many overloaded entities as needed, using concrete types.
\item C++ allows three kinds of template parameters:
\begin{enumerate}
    \item Type template parameter: represents a type.
    \begin{itemize}
        \item Use keywords \texttt{typename} or \texttt{class}.
        \item \texttt{typename} can be replaced by any type, primitive or a class.
        \item TODO: are the two equivalent?
    \end{itemize}
    \item Non-type template paramater: represents a constexpr value.
    \item Template template paramater: represents a template.
\end{enumerate}
\item Templates are analogous to stencils.
\item Templates can work with types that didn't even exist when they were written.
\item It's common convention to use uppercase letters for template parameters.
\end{itemize}
\subsection{Function Templates}
\begin{itemize}
\item Example:
\begin{lstlisting}
template<typename T>
T max(T x, T y){
    return (x<y)?y:x;
}
// signature is now: T max<T>(T x, T y);
\end{lstlisting}
\item A function call looks like: \texttt{max<int>(a,b);}
\item It is necessary to specify T in every call to \texttt{max<T>}.
\item When the compiler encounters a call to \texttt{max<int>}, and it doesn't have a corresponding definition yet,
it uses the template to create a function for the type selected.
\item This is known as \textbf{(Function) (Template) Instantiation}.
\item \textbf{Implicit instantiation} refers to instantiation due to a function call statement.
\item Instantiated functions are called a \textbf{function instance} or \textbf{template function}.
\item Function instances are normal functions in all regards.
\item Function instantiation is a compiler task, and thus happens during compilation.
\end{itemize}
\subsubsection{Template Argument Deduction}
\begin{itemize}
\item If all the template type parameters appear in the function's argument, and 
the arguments we pass to the function have the right type (without the need for coercion),
then we can omit the type specification in the function call statement:
\begin{lstlisting}
max<int>(a,b);
max<>(a,b);//Compiler only studies templates for finding an overload.
max(a,b);//compiler first studies non-template declarations of max, followed by templates. 
\end{lstlisting}
\item The order of checking for overloads before templates allows for type-specific optimizations in generic functions.
\item So, if the type parameters are deducible from the arguments and we want to use the type-specific, non-template
overload of the function, use \texttt{max(a,b)}, but use \texttt{max<>(a,b)} if you want to use the template overload
for some reason.
\item For instantiated functions to successfully compile, it is necessary for their bodies to not invoke any functions or operators not defined for the types being inferred from the function calls.
\item Note that the compiler generates instantiated functions so long as they make sense syntactically. It is our responsibility to ensure semantic sanity is maintained.
\item We can disallow particular template overloads using \textbf{function template specialization} like so:
\begin{lstlisting}
template <typename T>
T addOne(T x)
{
    return x + 1;
}

// Use function template specialization to tell the compiler that addOne(const char*) should emit a compilation error
template <>//empty type parameter list, NOT <char*>
const char* addOne(const char* x) = delete;
\end{lstlisting}
\item Attempt to use deleted function instances raises compilation faults.
\item It is common practice to put the entire template declaration and definition in header files, rather than just the declaration, as all cpp files need to know the template exists for their function calls to instantiate the relevant overloads from the template.
\item Functions implicitly instantiated from templates are implicitly inline; inline functions can be defined in multiple files, so long as their definitions are identical.
\item TODO: inline functions.
\item The template themselves are not inline, which is a concept that only applies to functions and variables.
\item Generic programming refers to the practice of using templates and associated template parameters (generic types).
\item There are some drawbacks to template programming:
\begin{itemize}
    \item Code bloat and slow compile times: As the compiler instantiates a function for each call with a unique set of arguments, even compactly written templates cause the two problems.
    \item Error messages from using templates are much more convoluted than regular error messages.
\end{itemize}
\end{itemize}
\subsubsection{Multiple Template Types}
\begin{itemize}
\item Note that a function call with different types being passed in place of T without explicitly mentioning \texttt{<T>} raises compilation errors.
\begin{lstlisting}
template <typename T>
T max(T x, T y)
{
    return (x < y) ? y : x;
}
int main()
{
    std::cout << max(2, 3.5) << '\n';  // compile error
}
\end{lstlisting}
\item The compiler doesn't know which overload to refer to and which argument to coerce.
\item Type coercion is done only in function overloads, \textbf{not in template argument deduction}.
\item Note about function overloading:
\begin{itemize}
\item This compiles but isn't sound:
\begin{lstlisting}
int mymax(int x,int y)...
mymax(2,2.5);//returns 2
mymax(2.1,2.5);//returns 2
\end{lstlisting}
\item This compiles and is sound:
\begin{lstlisting}
double mymax(double x,double y)...
mymax(2,1);//returns 2. Note that double arguments are trivially sound.
mymax(2,2.5);//returns 2.5.
\end{lstlisting}
\item This doesn't compile:
\begin{lstlisting}
int mymax(int x,int y)...
double mymax(double x,double y)...
mymax(2,2.5);//compiler error, ambiguous overload.
\end{lstlisting}
\end{itemize}
\item There are three ways to allow this mixing of types in templates.
\begin{enumerate}
\item Static Casting the arguments in a function call, without specifying the template type.
\begin{itemize}
\item Must use static cast \texttt{static\_cast<T>(var)} instead of dynamic casting \texttt{(type T)(var)}, which happens at runtime, because we need the type to be casted at compile time, to find the right template.
\end{itemize}
\item Just provide the template type and avoid the need for template argument deduction.
\item Declare function template types with multiple type parameters. (Discussed below.)
\end{enumerate}
\item Allow multiple types in the argument list and use auto to ensure arithmetic conversions don't lose data.
\begin{lstlisting}
template <typename T, typename U>
auto max(T x, U y)
{
    return (x < y) ? y : x;
}
//now this works for (T,T) and (T,U) instances.    
//C++ 20 introduces abbreviated function templates:
//Below is a shorthand for the above beast:
auto max(auto x, auto y)
{
    return (x < y) ? y : x;
}
//should be used only if x and y are allowed to be of different types.
\end{lstlisting}
\end{itemize}
\subsection{Non-type Template Parameters}
\begin{itemize}
\item Serve as placeholders for constexpr values passed in during a function call.
\item A non-type template parameter can have be one of:
\begin{enumerate}
    \item An integral type
    \item An enumeration type
    \item \texttt{std::nullptr\_t}
    \item A floating point type (C++ 20)
    \item A pointer or reference to:
    \begin{itemize}
        \item An object
        \item A function or member function.
        \item A literal class type (C++ 20).
    \end{itemize}
\end{enumerate}
\item Example of usage:
\begin{lstlisting}
bitset<8> b;
\end{lstlisting}
\item Syntax:
\begin{lstlisting}
template <int N> // declare a non-type template parameter of type int named N
void print()
{
    std::cout << N << '\n'; // use value of N here
}
\end{lstlisting}
\item On seeing a call to \texttt{print<5>()}, the compiler instantiates a print function with N=5.
\item One application is in creating data structures whose size is fixed at compile time.
\item One other application is in static asserts for values passed as non-type template parameters. For example, in a sqrt function.
\item Type conversion of non-int parameters is often performed:
\begin{lstlisting}
print<'c'>();//compiles with casting 'c' to int.
\end{lstlisting}
\item Though only some types of constexpr conversions are allowed:
\begin{itemize}
    \item Integral promotions (ex. char to int).
    \item Integral conversions (char to long or int to char).
    \item User-defined conversions (some custom class to int).
    \item Lval to Rval conversions.
\end{itemize}
\end{itemize}
\section{Exceptions from LearnCpp.com}
\begin{itemize}
\item The primary issue with error-handling without exceptions is that
the error-handling code ends up intricately linked to the normal control
flow of code; constraining both how the code is laid out and how errors can 
be reasonably handled.
\item Exceptions allow decoupling of error-handling code from the regular flow of control.
\item Three C++ keywords are used for this:
\begin{enumerate}
\item \texttt{throw} It is followed by an object of any kind (int/double/class).
\item \texttt{try} Encloses the block of code that can raise an error.
\item \texttt{catch} Specifies the object to be expected if an error is thrown and what to do with it.
\end{enumerate}
\item Each try block can have many catch blocks, each expecting a different 
type of object to be thrown, but must have at least one.
\item Execution resumes normally at the end of the catch block.
\item Exceptions of non-primitive types should be caught in const references to avoid unnecessary copies.
\item The variable name of a parameter can be excluded if it is not used inside the catch block.
Though it's type must be specified nonetheless.
\item The compiler searches for matching enclosing "catch" blocks for a thrown exception up the program,
until it is caught. If no catch block is found, the program fails with an exception error.
\item The compiler only ever casts derived classes of exceptions to their parents classes, but it never
casts between primitive types.
\item The catch block can serve one of four purposes:
\begin{enumerate}
\item Convey the caught error to the user in a neat way, say, by printing to the console.
\item Return a value or error code back to the current function's caller.
\item Throw another exception, to be caught by an outer try-catch block.
\item Terminate the program cleanly.
\end{enumerate}
\end{itemize}
\subsection{Stack unwinding}
\begin{itemize}
\item If the current function doesn't have a try-catch block enclosing the thrown exception,
the current searches down the call stack to find the first enclosing try-catch block.
\item On finding said block, the stack unwinds (locals are popped off) until the control can 
resume in the stack's state of the function that contains said catch block.
\item If no catch block is found all the way to main, the program terminates \textbf{without} unwounding the stack.
\item This may lead to trouble if some locals have non-trivial destructors.
\item The stack not being unwound allows retention of all the debug info related to the exception.
\item Behold the catch-all handler, that catches all exceptions:
\begin{lstlisting}
    try{
        //exception throwing code.
    }
    catch(int a){

    }
    catch(...){
        cout<<"All non-int exceptions lead here"<<endl;
    }
\end{lstlisting}
\item The catch-all handler should be the last block in the catch block chain,
to allow for specific error-handling as much as possible.
\end{itemize}
\subsection{Exceptions within classes}
\begin{itemize}
\item If a constructor of class A throws an exception, all the initialized members call their 
respective destructors, but ~A() is never called, as construction never succeeded.
\item This can lead to missed cleanup for resources that had been allocatd in A() before the exception.
\item However, if each resource that demands cleanup was declared inside a member of A, that had
its own destructor with the requisite cleanup code, the cleanup would go through.
\item However, creating a class to manage each resource, and be a member in A, isn't efficient.
\item STL provides classes that have destructors for cleanup:
\begin{itemize}
    \item \texttt{std::unqiue\_pointer<T>} : for pointers that are freed on destruction.
\end{itemize}
\end{itemize}
\subsection{Classes for Exceptions}
\begin{itemize}
\item Seeing as primitive types convey vague information about the error, it is
preferable to define classes that are meant to be thrown around.
\item A hierarchy of such classes is useful and often done.
\item Catch blocks with an argument of a base class also catch all errors of derived classes
without slicing, if the argument is a reference argument, and with slicing otherwise.
\item If the base catch block precedes the derived catch block, the base catch block executes.
\item Otherwise, the derived catch block executes.
\item Hence, catch blocks for derived classes should be written before (above) those for base classes.
\item All of the exceptions thrown by STL in C++ are of classes derived from \texttt{std::exception}.
\begin{lstlisting}
catch(std::exception &e){
    cout<<e.what()<<endl;//prints the error description.
}
\end{lstlisting}
\item Note that \texttt{e.what()} messages vary across compilers.
\item A common derived class to use is \texttt{std::runtime\_error("message for what")}.
\item We can of course further extend the STL class hierarchy in our codebase.
\item Rethrowing exceptions is a common practice.
\item If an exception is caught by an argument that is of a class less-derived than the exception,
the right way to rethrow the exception is \texttt{throw;} (an empty throw statement).
\item A statement like \texttt{throw baseException;} slices the derived exception class.
\end{itemize}
\subsection{Storing Exceptions}
\begin{itemize}
\item When an exception is thrown, its data is initially on the local stack.
\item If the stack needs to be unwound for it to be caught, the compiler copies
its data to memory location reserved for exceptions off the stack, until
a catch block is encountered.
\end{itemize}
\subsection{Function Try Blocks}
\begin{itemize}
\item One use case of function try blocks is when we want to catch
an exception thrown by the constructor of a base class, in the constructor of the 
derived class.
\begin{lstlisting}
class B : public A
{
public:
    B(int x) try : A{x} // note addition of try keyword here
    {
        //usual constructor stuff
    }
    catch (...) // note this is at same level of indentation as the function itself
    {
            // Exceptions from member initializer list or constructor body are caught here
            std::cerr << "Exception caught\n";
            throw; // rethrow the existing exception
    }
};
\end{lstlisting}
\item Note that function level catch blocks \textbf{for constructors} must throw an exception.
They are not allowed to resolve and just consume exceptions.
\item Implicit rethrows are called at the end of the catch-block, hence it is better to generally avoid
letting control get to the end of these catch blocks.
\begin{center}
\includegraphics[width=13cm]{rsrc/functiontrycatch.png}
\end{center}
\item We shouldn't use the catch block to clean up resources of a failed constructor, as the object was never created if an exception was thrown.
\end{itemize}
\subsection{Common Design Mistakes}
\begin{itemize}
\item Cleanup calls to resources established and used inside a try block should be outside the catch block, so they're called in normal execution, and in exceptional execution. This is assuming that control exits the catch block.
\begin{lstlisting}
try
{
    openFile(filename);
    writeFile(filename, data);
}
catch (const FileException& exception)
{
    std::cerr << "Failed to write to file: " << exception.what() << '\n';
}
// Make sure file is closed
closeFile(filename);
\end{lstlisting}
\item Additionally, the variables must be in scope for the cleanup calls, and must be declared outside the try block.
\item Or as discussed earlier, we can uses STL classes like \texttt{unique\_pointer<T>} or other resource
classes with destructors that perform the cleanup.
\item Generally abstain from using exceptions in destructors as an exception thrown during stack unwounding
"confuses" the compiler between continuing the unwound or handling the exception.
Instead, write the error to a log file.
\item It should be noted that exceptions come with performance costs such as:
\begin{enumerate}
    \item Mainly: the expensive operation of unwinding the stack on an exception and finding the suitable catch block.
    \item An heavier executable.
    \item The program runs slower due to additional checks.
\end{enumerate}
\item Some compilers implement \textbf{zero-cost exceptions}, where the runtime cost
 of exception-handling is zero for a no-exception scenario, but incurs large penalties
 in exceptional scenarios.
\end{itemize}
\subsection{Exception Specifications}
\begin{itemize}
\item Language mechanism to document what kind of exceptions a function can throw, in its declaration.
\item The \texttt{noexcept} keyword in a function declaration guarrantees that the function won't throw an exception.
\begin{lstlisting}
void doSomething() noexcept{
    //body that can't throw any exception out.   
}
\end{lstlisting}
\item If a \texttt{noexcept} function is overridden,
the overriding function should also be declared as \texttt{noexcept}.
\item If at runtime, an exception is emitted from a \texttt{noexcept} function,
\texttt{std::terminate()} is called immediately, even if there exist handlers around
the function call.
\item The promise of \texttt{noexcept} is contractual; it is not enforced by the compiler.
\item Functions differing only in there exception values (like those differing only in their 
return values) can not be overloads of each other; they are all the same.
\item The noexcept operator can be used to store variables conveying whether a function or
expression can potentially throw errors or not:
\begin{lstlisting}
void foo() {throw -1;}
void boo() {};
void goo() noexcept {};
struct S{};
constexpr bool b1{ noexcept(5 + 3) }; // true; ints are non-throwing
constexpr bool b2{ noexcept(foo()) }; // false; foo() throws an exception
constexpr bool b3{ noexcept(boo()) }; // false; boo() is implicitly noexcept(false)
constexpr bool b4{ noexcept(goo()) }; // true; goo() is explicitly noexcept(true)
constexpr bool b5{ noexcept(S{}) };   // true; a struct's default constructor is noexcept by default
\end{lstlisting}
\end{itemize}
\section{The Inlined}
\subsection{Global Constants}
\begin{itemize}
\item Some constants might be used across files and we want to define them only once.
\item One way of doing this (pre C++17) was:
\begin{lstlisting}
#ifndef CONSTANTS_H
#define CONSTANTS_H
// define your own namespace to hold constants
namespace constants
{
    // constants have internal linkage by default
    constexpr double pi { 3.14159 };
    constexpr double avogadro { 6.0221413e23 };
    constexpr double myGravity { 9.2 };
}
#endif
\end{lstlisting}
\item And we \texttt{\#include<constants.h>} wherever we need the constants.
\item Each separate object file that uses \texttt{constants.h} gets a separate copy
of the constants, but the compiler removes them away later.
\item Downsides of this:
\begin{enumerate}
\item Each object file using \texttt{constants.h} has to be recompiled whenever the constants are updated.
\item If the constants aren't optimized away by the compiler for some reason and are large,
they use a lot of memory.
\end{enumerate}
\item The alternative is to:
\begin{enumerate}
\item Put forward declarations of the constants in a header file that can be included in multiple files.
\item Define them only once in a .cpp file.
\end{enumerate}
\item Note: \texttt{constexpr} variables can't be forward declared, as the compiler must know their values at compile time. We use \texttt{const} in this case:
\begin{lstlisting}
/*constants.cpp contents:*/
#include "constants.h"

namespace constants
{
    // actual global variables
    extern const double pi { 3.14159 };
    extern const double avogadro { 6.0221413e23 };
    extern const double myGravity { 9.2 }; // m/s^2
}

/*constants.h contents*/
#ifndef CONSTANTS_H
#define CONSTANTS_H

namespace constants
{
    // since the actual variables are inside a namespace, the forward declarations need to be inside a namespace as well
    extern const double pi;
    extern const double avogadro;
    extern const double myGravity;
}
#endif
\end{lstlisting}
\item Downside of this method is that the values of \texttt{constants::pi} and the others aren't treated as constants in files other than \texttt{constants.cpp}. This keeps several helpful optimizations from happening.
\item \texttt{inline} variables are a better solution from \texttt{C++ 17}
\item An inline variable, declared a single header file, can be used in multiple cpp files without generating redundant copies.
\item Inline global variables have external linkage by default.
\end{itemize}

\section{Unions}
\href{https://en.cppreference.com/w/cpp/language/union}{Cpp Reference Page}
\section{Misc. (Courtesy of Sony)}
\begin{itemize}
\item \texttt{cout.width(len)} prefixes the next output (with whitespace) sent to it so that its length is equal to len
\item If len < output.size(), the function call has no effect.
\end{itemize}
\begin{itemize}
\item Size of objects of a union class is the maximum size of any one field.
\end{itemize}
\section{Between Interviews}
\subsection{C++ padding/packing}
\begin{itemize}
\item The size of a struct (and class) is not always just the sum of the sizes of the data members. This is attributed to padding and packing.
\begin{lstlisting}
struct Base{
    char a;
    char b;
    char c;
    int i;// sizeof(Base) returns 8.
};
struct Base{
    char a; 
    char b;
    int i;
    char c;//sizeof(Base) returns 12
    // Memory layout: a|b|-|-|i|i|i|i|c|-|-|-
};
\end{lstlisting}
\item \texttt{\#pragma pack(1)} disables padding.
\end{itemize}
\section{C++ Qs}
\begin{itemize}
\item \texttt{mutable} makes a data member modifiable even inside const member functions.
\end{itemize}
\chapter{DSA Revision Notes}
\section{Stacks, Queues, Linked Lists}
\begin{itemize}
\item Implementation of stack, queue, deque usually involve maintaining a
fixed-size array and a pointer to the top/front/back of the queue that moves around.
\item The growth strategies for a stack's underlying array are:
\begin{itemize}
    \item Growth: f(N) = 2N;
    \item Tight: f(N) = N+c;
\end{itemize}
\item Push operation that call growing the underlying array cost f(N)+N+1 units.
\end{itemize}
\subsection{Queues}
\begin{itemize}
\item A common use of queues (or deques) is to explore a search space in steps, for example BFS.
\item Each step involves expanding the frontier of the space by one unit, while popping the previous frontier from the queue.
\end{itemize}
\section{Hash Tables}
\begin{itemize}
\item Load factor $\alpha = n/m$, where the hash table has $m$ slots (unique indices)
and holds $n$ elements. $\alpha = $ average number of elements in each slot.
\item A hash function maps the keys of a hash table to its indices.
\item Hash functions are the composition of:
\begin{enumerate}
    \item Hash codemap: $KeySet \Longrightarrow Z$
    \item Compression map: $Z \Longrightarrow [0,1,...N-1]$
\end{enumerate}
\item Both have to deterministic and dependent on the key.
\item Strings are usually hashed using polynomials, with the characters
as coefficients. Common points of evaluation of polynomials are
x = 33, 37, 39 or 41.
\item Some common compression maps:
\
begin{itemize}
    \item $h(z) = z \% m$, where m is prime (~not too close to powers of 2.) m=$b^e$ makes for a bad compression map.
    $x \% 2^e$ gives last e bits of x.
    \item $h(z) = floor(m(fractional(zA)))$, where A $\in$ (0,1) and m = size of the table.
    \item Often use m = $2^p$ for above.
    \item Fibonacci hashing involves using A = $\frac{\sqrt{5}-1}{2}$
    \item $h(z) = |ak+b| mod m$ where a,N are coprime.
\end{itemize}
\subsection{Universal Hashing}
\begin{itemize}
\item A solution to the idea of adversarial choices of elements for a determined hash function.
\item A collection H of hash functions is universal if \\
For a randomly chosen h $\in$ H and two keys $k$ and $l$,\\
$P(h(k)=h(l)) < 1/m$
\end{itemize}
\subsection{Collision Resolution}
\subsubsection{Chaining}
\begin{itemize}
\item Each key is mapped to a linked list with elements that share the key.
\item Chaining is the most time efficient collision resolution method.
\item Linked lists can be sorted if necessary.
\item Search time = O($\alpha$).
\end{itemize}
\subsubsection{Open Addressing}
\begin{itemize}
\item Allows storage of at most m elements.
\item Introduce NULL elements. Use an array of size m, initialized to NULLs.
\item Systematically probe slots when searching for an element.
\item Modify the hash function to take the probe number i as the second parameter.\\
$h:\Longrightarrow Keyset \times {0, 1, 2 ... m-1}$
\item Hash function determines the sequence of slots to be examined for a given key.
\item ${h(k,0), h(k,1)...h(k,m-1)}$ is a permutation of ${0,1, ... m-1}$
\item Variations of probing include:
\begin{enumerate}
    \item Linear probing
    \begin{itemize}
        \item if h(k) is used, find next empty slot for insertion.
        \item Use a tombstone marker to delete elements, so later search queries can read the tombstone and check further elements.
        \item For deletion, search all next slots where h(slot value) = h(k) or slot value == tombstone.
        \item Rehash if there are too many tombstones.
        \item Inserts are allowed to use tombstones.
    \end{itemize}
    \item Double hashing
    \begin{itemize}
        \item set h1(k) = initprobe and h2(k) = offset.
        \item Search for empty slots starting at initprobe, in offsets of h2(k) mod m.
        \item Avoid functions h2(k) which can be give zero.
        \item If m is prime, this method examines every slot in the table.
        \item Expected number of probes to find an empty slot = $\frac{1}{1-\alpha}$ (Or an unsuccessful search.)
    \end{itemize}
\end{enumerate}
\end{itemize}
\section{Trees}
\begin{itemize}
\item In a tree, degree of a node may refer to the number of children it has.
\item \textbf{Binary Tree}: An ordered tree with each node having at most two children.
\item \textbf{Complete Binary Tree}: Each level i has $2^i$ nodes, i = 0, 1, ... h.
\begin{itemize}
    \item \# Leaves = $l$ = $2^h$; Total nodes = n = $2^{h+1} - 1$; \# Internal nodes = m = $2^{h}-1 = l - 1$
    \item height = h = $log_2(l) = log_2((n+1)/2)$
\end{itemize}
\item If a binary tree has n nodes,
\begin{equation*}
n \le 2^{h+1} - 1 \implies n-1 \le h \le log_2((n+1)/2)
\end{equation*}
\begin{equation*}
1 \le l \le m+1, l = n-m \implies l \le \frac{(n+1)}{2}
\end{equation*}
\end{itemize}
\subsection{Binary Search Tree}
\begin{itemize}
\item Structure used for ordered dictionaries, where the keys are stored
in a binary tree.
\begin{lstlisting}
Tree search(Tree t,Value v){
    if(t->value==v){
        return t;
    }
    else if(t->value < v){
        if(t->left){
            return search(t->left,v);
        }
        return NULL;
    }
    else{
        //t->value > v
        if(t->right){
            return search(t->right,v);
        }
        return NULL;
    }
}
\end{lstlisting}
\item Inorder traversal of a BST gives a sorted projection of all the keys. This is called BST-sort.
\item Has the same complexity cases as quick-sort:
\begin{itemize}
    \item WCS: O($n^2$)
    \item BCS: O($nlogn$)
    \item Average over $n!$ permutations: O($nlogn$)
\end{itemize}
\item Method implementations:
\begin{enumerate}
\item Replace: Given x,y and parent(x), replace x by y as a child of parent(x).
\begin{lstlisting}
void replace(Node*x, Node*y, Node*px){
    if(px->left==x){
        px->left = y;
    }
    else{
        //(px->right==x)
        px->right = y;
    }
}
\end{lstlisting}
\item Search: as given above.
\item Insert: search for node in tree, insert as a child where NULL is found.
\item Successor:
\begin{lstlisting}
Node *n successor(Node *x, Node *n){
    if(x->right){
        return leftMostChild(x->right);
    }
    else{
        Node *y = parent(x,n);
        while(y!=NULL && x==y->right){
            x = y;
            y = parent(x,n);
        }
        //y == root or y is closest ancestor such that x is in y->right.
        return y;
    }
}
\end{lstlisting}
\item Deletion:
\begin{lstlisting}
void delete(Node *x, Node *n){
    if(!(x->left&&x->right)){
        //x has at most one child.
        Node *y = parent(x,n);
        if(y->left==x){
            y->left = oneChildOf(x);//may return null if x has no children.
        }
        else if(y->right==x){
            y->right = oneChildOf(x);
        }
    }
    else{
        //x has two children.
        Node *s = Successor(x,n);
        delete(s,n);
        Node *y = parent(x,n);
        //transfer the children
        s->right = x->right;s->left = x->left;
        if(y->left==x){
            y->left = s;
        }
        else if(y->right==x){
            y->right = s;
        }
    }
}
\end{lstlisting}
\end{enumerate}
\end{itemize}
\subsection{Traversals}
\begin{enumerate}
\item Pre-order: Do me pre my children; process each node before its children are processed.
\begin{lstlisting}
void preorder(Node n){
    process(n);
    for(Node &u:n.children){
        preorder(u);
    }
}
\end{lstlisting}
\begin{itemize}
    \item Used in reading documents, webpages.
\end{itemize}
\item Post-order: Do me post my children; process each node after its children are processed.
\begin{lstlisting}
void postorder(Node n){
    for(Node &u:n.children){
        postorder(u);
    }
    process(n);
}
\end{lstlisting}
\begin{itemize}
\item Used by the disk-usage command and evaluation of arithmetic expressions.
\end{itemize}
\item In-order: Do me between my children; process left children first,
followed by the given node, followed by the right children.
\begin{lstlisting}
void inorder(Node n){
    for(Node &u:n.leftchildren){
        inorder(u);
    }
    process(n);
    for(Node &u:n.leftchildren){
        inorder(u);
    }
}
\end{lstlisting}
\item Eulerian: Generic traversal combining pre-, post- and in-order traversals.
\begin{lstlisting}
void eulerian(Node n){
    process(n);//First process
    for(Node &u:n.leftchildren){
        eulerian(u);
    }
    process(n);//Second process
    for(Node &u:n.leftchildren){
        eulerian(u);
    }
    process(n);//Third process.
}
\end{lstlisting}
\end{enumerate}
\subsection{AVL Trees}
\begin{itemize}
\item They are aka height-balanced binary search trees.
\item If $h(n)$ denotes the height of the tree rooted at n:
\begin{center}
    $h(n) = 1+max(h(n\rightarrow left),h(n\rightarrow right))$\\
    $h(n) = 1 \implies $ n is a leaf.\\
\end{center}
\item Define the balance factor of a node as:
\begin{center}
    $BF(n) = h(n\rightarrow right) - h(n\rightarrow left)$
\end{center}
\item $\forall n \in \{Internal Nodes\}, |h(n\rightarrow right)-h(n\rightarrow left)| \le 1$
\item $h(root) \le log_{\phi}(n)$
\item If the leaf closest to the root is at level k:
\begin{enumerate}
    \item $h(root) \le 2k-1$
    \item All nodes at levels 1...k-2 have 2 children.
    \item 2 $\implies$ n $ \ge 2^k - 1 $
    \item 1, 3 $\implies 2^{k-1} \le n \le 2^{2k-1}$
    \item 4 $\implies 2^{(h-1)/2} \le n \le 2^{h}$
\end{enumerate}
\item In an AVL tree of height h,
\begin{enumerate}
    \item The leaf closest to the root is at level $\ge (h+1)/2$.
    \item On the first $(h-1)/2$ levels, AVL tree is complete (full).
    \item $ 2^{(h-1)/2}\le n \le 2^{h}$
\end{enumerate}
\item Method implementations:
\begin{enumerate}
\item Update: (private) method to update height values and balance factor values across the tree (or subtree passed as argument.)
\item Rotaion: operation used in insertion and deletion for balance restoration.
\begin{center}
\includegraphics[width=8cm]{rsrc/AVL-rotation.jpg}
\end{center}
\begin{itemize}
\item Either structure being a valid BST implies the other is a valid BST.
\end{itemize}
\begin{lstlisting}
void rotate(Node* u,Node* v){
    //u is the parent to v.
    Node *p = parent(u);
    if(u->left==v){
        u->left = v->right;
        v->right = u;
        if(p->left==u){
            p->left = v;
        }
        else{
            p->right = v;
        }
    }
    else{
        u->right = v->left;
        v->left = u;
        if(p->left==u){
            p->left = v;
        }
        else{
            p->right = v;
        }
    }
}
\end{lstlisting}
\item Insertion: O(logn)
\begin{lstlisting}
void insert(Node *n, Value x){
    PathTraversed path = bstInsert(n,x);
    //path.last = x's node.
    //find closest ancestor of x that is imbalanced.
    //imbalance is possible because of a height increment.
    Node *top, *mid, *bot;
    bot = path.back();//x's node.
    mid = parent(bot);
    top = parent(mid);
    //top will point to the imbalanced node.
    //top->mid->bot belongs to the path.
    while(balanced(top) && parent(top)!=top){
        //root check condition.
        path.pop_back();
        back = mid;    
        mid = top;
        top = parent(top);
    }
    if(!balanced(top)){
        if(BF(top)*BF(mid)>=0){
            //left-left or right-right cases.
            rotate(top,mid);
            //balanced.
        }
        else if(BF(top)*BF(mid)<0){
            //left-right or right-left cases
            rotate(mid,bot);
            rotate(top,bot);
            //balanced.
        }
    }
}
\end{lstlisting}
\item Deletion: O(logn) We follow a similar strategy to deletion in BSTs, followed by recursive balancing.
\begin{lstlisting}
void delete(Node*n, Node* x){
    Node *z = parent(x,n);
    //if its the root, the pointer is modified.
    if(!(x->left && x->right)){
        replace(x,oneChildOf(x),z);
    }
    else{
        //x has two children.
        Node *s = successor(n,x);
        delete(n,s);
        replace(x,s,parent(x));
        z = parent(s,n);
    }
    while(parent(z)!=z){
        while(balanced(z) && parent(z)!=z){
            //find first imbalanced ancestor until root.
            z = parent(z);
        }
        y = childWithLargerHeight(z);
        x = childWithLargerHeight(y);
        if(BF(z)*BF(y)>=0){
            rotate(z,y);
        }
        else{
            rotate(y,x);
            rotate(z,x);
        }
    }
}
\end{lstlisting} 
\begin{itemize}
\item Insertion is quicker because there's only one point where rebalancing is necessary, whereas in deletion, the imbalance may be transferred higher up the tree.
\end{itemize}
\end{enumerate}
\end{itemize}
\section{String Stuff}
The common problem of finding a pattern P in a text T (|T| > |P|)
can be solved efficiently by:
\begin{enumerate}
    \item Either preprocessing the pattern (KMP)
    \item Or preprocessing the text (Tries)
\end{enumerate}
The latter is preferred if multiple patterns are searched for a in a single piece of text.
\subsection{KMP}
\begin{lstlisting}
vector<int> kmp(string t, string p){
    int l = p.size();
    int n = t.size();
    //this lps implementation is inefficient. O(l^3).
    vector<int> lps(l,0);//lps[0,1,...l-1].
    //lps[0] = 0.
    for(int i=2;i<=l;i++){
        for(int j=1;j<i;j++){
            if(p.substr(0,j)==p.substr(i-j,j) && (i==l || (p[j]!=p[i]))){
                lps[i-1] = max(lps[i-1],j);
            }
        }
    }
    int j = 0, i=0;
    vector<int> matches = {};
    for(;i<l && j<n;){
        if(t[j]!=p[i]){
            //mismatch
            if(i==0){
                j+=1;
            }
            else{
                i = lps[i-1];
            }
        }
        else{
            //match=>increment both.
            i++;
            j++;
            if(i==l){
                i = lps[i-1];
                matches.push_back(j-l);
            }
        }
    }
    return matches;
}
\end{lstlisting}
\subsubsection{Linear Time LPS}
It's complicated man. Maybe do this later.
\subsection{Tries}
\begin{itemize}
    \item Tries are meant to store data indexed by strings from a fixed set of characters.
    \item Said data can be the occurrences of the index string in a large body of text.
    \item A generic implementation is give below:
    \begin{lstlisting}
    class Trie{
        int charindex(char c);//each char mapped to an integer,
        vector<Trie*> chars = vector<Trie*>(numChars,NULL);//integers indexing a Trie pointer,
        Data data;//initialized to empty.
        void insert(string index,Data data){
            int i = 0;
            Trie* t = this;
            while(i<index.size()){
                if(!t->chars[charindex(index[i])]){
                    t->chars[charindex(index[i])] = new Trie;
                }
                t = t->chars[charindex(index[i])];
                i++;
            }
            t->data = data;
        }
        Data* get(string index){
            int i = 0;
            Trie* t = this;
            while(i<index.size()){
                if(!t->chars[charindex(index[i])]){
                    return NULL;
                }
                t = t->chars[charindex(index[i])];
                i++;
            }
            return &(t->data);
        }
        void remove(string index){
            int i = 0;
            Trie* t = this;
            while(i<index.size()){
                if(!t->chars[charindex(index[i])]){
                    return;
                }
                t = t->chars[charindex(index[i])];
                i++;
            }
            return t->data = NULL;
        }
    };
    \end{lstlisting}
    \item This implementation's space complexity is O(|W|J), where J is the number of unique
    ordered pairs (i,c) such that data has been indexed using a string which has c at index i.
    W is the character set.
    \item A more space-efficient, and thus access time-inefficient implementation would be
    to use maps from chars to Trie pointers.
    \item A suffix Trie can be constructed in O(|T|) time, which means pattern matching can be done
    in O(|P|+|T|+k) time, where k is the number of times the pattern occurs in T.  
    \end{itemize}
\section{Heap}
\begin{itemize}
\item Implemented as a binary tree to order items by a parameter.
\item All levels except the last one are full. The last level is left-filled.
\item Structural Property: $\forall$node, Priority(node)$\le$Priority(node.parent)
\item $\implies$ the root has the maximum priority.
\item Height of a heap with n nodes is $h = floor(log_2(n+1))$
\end{itemize}
\subsection{Array Implementation}
\begin{lstlisting}
vector<int> a(vals.begin(),vals.end());
//zero indexed.
int parent(i){
    return (i-1)/2;
    //parent(0) returns 0.
}
int left(i){
    return 2*i + 1;
}
int right(i){
    return 2*i + 2;
}
//condition: for all i, a[i] <= a[parent(i)]
\end{lstlisting}
\subsection{UpwardsHeapify}
\begin{lstlisting}
void upwardsHeapify(int i){
    while(i!=0){
        //MAX HEAP => parent should be >. Swap if not.
        if(a[parent(i)] < a[i]){
            swap(a[parent(i)],a[i]);
        }
        i = parent(i);
    }
}
\end{lstlisting}
\subsection{Insertion}
\begin{lstlisting}
void insert(int v){
    a.push_back(v);
    upwardsHeapify(a.size()-1);
    return;
}
\end{lstlisting}
\subsection{Heapify}
\begin{itemize}
\item Function used to convert any given binary tree into a heap.
\end{itemize}
\begin{lstlisting}
heapifyNode(int i){
    int vl=INT_MIN,vr=INT_MIN;
    if(left(i)<n){
        vl = a[left(i)];
    }
    if(right(i)<n){
        vr = a[right(i)];
    }
    if(a[i] < max(vl,vr)){
        if(a[left(i)] > a[right(i)]){
            swap(a[i],a[left(i)]);
            heapifyNode(a[left(i)]);//continues down the tree.
        }
        else{
            swap(a[i],a[right(i)]);
            heapifyNode(a[right(i)]);//continues down the tree.
        }
    }
}
heapifyTree(int i){
    if(i<0 || i > n-1){return;}
    heapifyTree(left(i));
    heapifyTree(right(i));
    heapifyNode(i);
}
\end{lstlisting}
\begin{itemize}
\item Note that heapify continues down the tree, because the branch whose head is swapped with the given node is not guarranteed to be a heap with the new node.
\item \texttt{heapifyNode} takes O(logn) time.
\item \texttt{heapifyTree} takes O(nlogn) time, since it visits each node once, calling \texttt{heapifyNode}.
\end{itemize}
\subsection{Deletion (of root)}
\begin{lstlisting}
void deleteMin(){
    arr[0] = arr[arr.size()-1];
    heapifyNode(0);//Note: deletion is O(logn)
    return;
}
\end{lstlisting}

\subsection{Inplace Heapsort}
\begin{itemize}
\item Works in O(nlogn) time and O(1) space.
\item The key idea is to implement max heap helper functions and keep swapping a[0] with a[n-1], reducing n each time.
\end{itemize}
\begin{lstlisting}
void inplaceheapsort(vector<int>& nums){
    int n = nums.size();
    vector<int> ans;
    maxHeapifyTree(nums,0,n);
    while(--n){
        swap(nums[0],nums[n]);//max elem moved to end of array repeatedly.
        maxHeapifyNode(nums,0,n);
    }
    return;
}
\end{lstlisting}
\subsection{Other modifications}
\begin{itemize}
\item If an element in a max heap is incremented, we must call \texttt{upwardsHeapify} to ensure the array remains a heap; if it is decremented, we must call \texttt{heapify}. Vice-versa for elements in min heaps. 
\end{itemize}
\subsection{Array to Heap in O(n)}
\begin{itemize}
\item \texttt{heapifyTree(0)} and repeated insertions run in O(nlogn) time.
\item A linear solution is below:
\begin{lstlisting}
//Note: only first half of the elems are used in the loop
for(int i=n/2;i>-1;i--){
    heapifyNode(i);
}
\end{lstlisting}
\item The running time is linear because the heapify calls incur low costs for of the nodes.
\item In particular, there are at most $n/2^i$ nodes that incur $i$ swaps (or operations) in heapify.
\item The sum: $\sum_{i=1}^{logn}{n\times i/2^i}$ is strictly bounded by $2n$
\end{itemize}
\chapter{Operating Systems}
\href{C:/CODE/atgit/notes/osrev/osrev.pdf}
{Find separate notes here.}
\chapter{Networks}
\href{C:/CODE/atgit/notes/networks/networks.pdf}
{Find separate notes here.}
\chapter{DBMS}
\section{\href{https://www.sqltutorial.org/sql-cheat-sheet/}{SQL}}
\begin{itemize}
\item Notation:
\begin{itemize}
    \item \texttt{T1, T2, ...}: Data types
    \item \texttt{A1, A2, ...}: Column names and \texttt{A}: entire set of columns.
    \item \texttt{a1, a2, ...}: An example of values for A1, A2,... and \texttt{a}: a set of values of each column.
    \item \texttt{R1, R2, ...}: Table (relation) names.
    \item \texttt{E1(A), E2(B), ...}: Expressions using column names from \texttt{A} and \texttt{B}.
    \item \texttt{C1, C2, ...}: Condition statements.
    \item \texttt{abc|def}: either abc or def can be used here.
\end{itemize}
\item Select statements:
\begin{lstlisting}[caption=The Usual]
select (A1, A2, A3) from R1 where C1;
\end{lstlisting}
\begin{itemize}
\item Examples of C1:
\begin{lstlisting}
A1 = a1
A1 > a1
A1 < a1
A1 <= a1
A1 >= a1
A1 <> a1
A1 LIKE pattern
A1 in (a1,a1',a1'')
A1 between a1 and a2
A1 IS NULL
\end{lstlisting}
\item Each condition operand can be preceeded by NOT to negate it. Ex: \texttt{A NOT LIKE pattern}.
\item In patterns, \texttt{\_} replaces 1 character and \texttt{\%} replaces 0, 1 or more characters.
\end{itemize}
\begin{lstlisting}[caption=Variations of Select]
select (A1, A2, A3) from R1 order by A1,A2 desc|asc;
select distinct (A1, A2, A3) from R1 where E1(A);
select (A1, A2, A3) from R1 limit n offset k;//skip first k rows and return the next n rows.
select A1, aggregate(A2) from R1 group by A1;
select A1, aggregate(A2) from R1 group by A1 having E1(aggregate(A2));
\end{lstlisting}
\item Joins:
\begin{enumerate}
\item \texttt{(Inner) Join}: return rows where both tables have an entry for the common column (ignore matchless columns from both tables).
\item \texttt{Left (outer) Join}: return all rows from table 1 (on the left), and if the row's column has a matching row in table 2, append its data too. 
\item \texttt{Right (Outer) Join}: return all rows from table 2 (on the right), and if the row's column has a matching row in table 1, append its data too.  
\item \texttt{Full (Outer) Join}: return all rows from both tables, correlating the ones with matching entries in the common column. 
\item cross join: every row is joined with every row. This is a costly operation and is rarely used. It is invoked in the following query:
\begin{lstlisting}
select A1,A2 from R1,R2;
\\equivalent to
select A1,A2 from R1 cross join R2;
\end{lstlisting}
\item The generic join sytax is:
\begin{lstlisting}
select * from R1 (join-specification) R2 on C;
\end{lstlisting}
\end{enumerate}
\item Set operations can be used to combine the results from two separate select queries:
\begin{lstlisting}[caption=Set Operation]
select * from R1 UNION select * from R2;
select * from R1 INTERSECT select * from R2;
select * from R1 MINUS select * from R2;
\end{lstlisting}
\end{itemize}
\chapter{(CPU) Architecture}
From The Art of Writing Efficient Programs.
\section{Introduction}
\begin{itemize}
\item Our program will deliver the result faster if every computation it executes brings us closer to the result and, at every moment, we do as much computing as possible.
\item We want to study here how to make the best use of the CPU resources using a single thread.
\item Some components on the CPU largely work by themselves and there is little the programmer needs to do the make the best use of them.
\item Some need a careful arrangement of machine code that is mostly done by compilers.
\item But more than half the chip is dedicated to components that just don't optimize themselves and require the programmer's understanding and tweeking to function optimally.
\mygraphic{rsrc/silicon.png}
\end{itemize}
\section{Benchmarking Details}
\begin{itemize}
\item Division is much slower (3 to 4 times) than addition, subtraction and multiplication (which are roughly of the same order).
\item When operating on a two arrays and reading their elements, the values first have to be loaded into registers before the operation can be executed. (Or, in some cases, one of the values has to be loaded while the other can be referenced in memory).
\begin{lstlisting}
a1 = v1[i]+v2[i]; //for i in a range,
//broken into:
/*
read v1[i] into r1;
read v2[i] into r2;
r3 = r1+r2;//use r3 for a1 operations ahead.
*/
\end{lstlisting}
\item Note that the below program has the same cost (~total cycles or time) as the one above:
\begin{lstlisting}
a1 = v1[i]+v2[i]; //for i in a range,
a2 = v1[i]*v2[i]; //for i in a range,
a3 = v1[i]<<v2[i]; //for i in a range,
\end{lstlisting}
\item Once the operands (v1[i], v2[i]) are loaded into the registers, the processor can execute several operations at once. This is known as instruction-level parallelism (ILP).
\item There is a limit to how many such instructions the CPU can do at once (even after loading the values into the registers).
\item We can use a tool like Machine Code Analyzer (MCA) to get information on how the instructions are executed, where the delays and bottlenecks are among other details.
\end{itemize}
\section{Data Dependencies and Pipelining}
\begin{itemize}
\item Complex arithmetic instructions, when broken down into 3A-instructions, tend to have some instructions dependent on the completion of the ones before them.
\begin{lstlisting}
a1[i] = (p1[i]+p2[i])*(p1[i]-p2[i]);//one iteration of the loop
//breaks into:
s[i] = (p1[i]+p2[i]);
d[i] = (p1[i]-p2[i]);
//s,d evaluations can employ ILP
a1[i] = s[i]*d[i];//can't be executed unless we have s[i] and d[i].
\end{lstlisting}
\mygraphic{rsrc/datadependency.png}
\item The iteration now takes two cycles instead of one, because results of the first cycle are used in the second.
\item One solution to this is pipelining the asm code to compute a1[i-1] after computing s[i],d[i]:
\mygraphic{rsrc/pipelining.png}
\item Stage 2 of the previous expression runs at the same time as stage 1 of the next instructions. (The pipeline gets deeper for more complex instructions.)
\item With more iterations, while the first iteration takes two cycles, subsequent iterations are effectively done in one cycle, and the time should be comparable to that of an iteration without data dependency.
\item Note: this pipeling is done by the hardware, not by the compiler. The compiler simply generates the code for one iteration and the operations needed to advance to the next iteration.
\item Register renaming, where the CPU maps register names written in asm \texttt{rsi, rax} etc to physical registers, helps avoid the contradiction of s[i-1] and s[i] using the same register and therefore requiring disjoing cycles.
\item For example, in this two-level pipeline, \texttt{rsi} is mapped to two separate registers, distinguished by the CPU as \texttt{rsi1} and \texttt{rsi2}. This hardware renaming is not visible to any tools like LLVM-MCA or a profiler.
\item This register renaming allows for the pipelining discussed above.
\item \textbf{Loop unrolling:} refers to converting a loop into linear code. It is a popular compiler optimization technique, but here, it is done in the hardware.
\item Observation: The order in which the CPU executes our code is not the same order in which the instructions are written. This is called out of order execution, and has important consequences for multi-threaded programs.
\end{itemize}
\section{Pipelining and Branches}
\begin{itemize}
\item Conditional execution (branches) are the bane of pipelining, much like data dependency is the bane of ILP.
\item The instruction \texttt{a1 = p1[i]>p2[i]?p1[i]:p2[i]} is broken down as:
\mygraphic{rsrc/branches.png}
\item This has a data dependency as a code dependency. We need the value of v1[i]>v2[i] before deciding which of the two instructions to execute.
\item Having conditional move or conditional add instructions in the ISA can maintain the same performance:
\mygraphic{rsrc/cmove.png}
\item With this, our code becomes entirely sequential, without any jumps, and can be pipelined perfectly.
\item Though not all compilers use them to implement the ternary operator.
\item The conditional code discussed in the book is about five times slower than the sequential code.
\end{itemize}
\section{Branch Prediction}
\begin{itemize}
\item Consider a branchless, hardware-pipelined loop:
\begin{lstlisting}
for (size_t i = 0; i < N; ++i) {
    a1 += v1[i] + v2[i]; // s[i] = v1[i] + v2[i]
}
\end{lstlisting}
\item The processor perceives the corresponding instructions as:
\mygraphic{rsrc/wloop.png}
\item Ideally, w is such that at every cycle, the CPU is executing exactly as many instructions as it can simultaneously (rarely possible in practice).
\item However, there's no guarrantee that there are w elements ahead. While evaluating v1[i]+v2[i], i may be N-1, and loading v1[i+2] would result in UDB.
\item Hence, we must check if i < N even in the branchless code, before we execute the pipelined set of instructions. The code is therefor conditional (has branches).
\item The way to save pipelining in the presence of branches is to attempt to convert the conditional code into an unconditional sequence, by trying to predict what branch will be taken.
\item For example, if N is large, i<N will evaluate to true for most iterations.
\item The processor makes a bet on facts like these using branch prediction.
\item It analyzes the history of every branch in the code and assumes that the behaviour will not change in the future.
\item Of course, we defer the actual writing of the results into memory until we evaluate the condition and confirm that the iteration does, in fact, happen. The processor has some write buffers to hold such unconfirmed results before committing them to memory.
\item \textbf{Speculative Execution:} execution of code in a branch before we know that this code really exists (the branch is to be taken).
\item If the prediction is wrong, the processor has to:
\begin{enumerate}
    \item Discard the result of every instruction that should not have been evaluated.
    \item Fetch the instructions that it previously assumed wouldn't be needed and evaluate them instead.
\end{enumerate} 
\item The above event is called a pipeline flush and it is an expensive occurrence.
\item The benchmarks show that cost of a well-predicted branch is minimal and that it is much faster than exactly the same code with a branch that is predicted poorly (say, based on random numbers).
\item Most profilers can profile branch prediction failures, and pinpoint the functions where these mispredictions happen.
\item Branch misses \~ 0.09\% is much better than \~ 11.17 \%.
\item Note that predictors can:
\begin{itemize}
    \item Distinguish between different call sites leading to different prediction patterns and make use of this.
    \item Recognize complex patterns, like an alternating boolean value. 
\end{itemize}
\end{itemize}
\section{Speculative Execution}
\begin{itemize}
\item In the case of a misprediction, we may end up reading values outside of the valid memory region. (Ex: v1[N+1] in the previous example.) Even reading such values can result in UDB.
\item Another example is the function below:
\begin{lstlisting}
int f(int* p) {
    if (p) {
            return *p;
    } else {
            return 0;
    }
}
\end{lstlisting}
\item Here, a null pointer may be dereferenced in the case of a misprediction.
\item Such errors, and incorrect writes (discussed earlier), and any action that cannot be undone are treated as \textbf{potential actions}, as long as the instruction that caused that action remains speculative (until the condition is evaluated).
\item The CPU has special hardware circuits, such as buffers, to store these events temporarily, while the condition is evaluated.
\end{itemize}
\section{Optimizations}
\begin{itemize}
\item Replacing compound boolean expressions (\texttt{c3 || (c1 \&\& c2)}) by corresponding expressions that use arithmetic operations or bitwise operations (and maintain program correctness) helps improve performance if:
\begin{itemize}
    \item The compound expression often evaluates to a single value (true/false).
    \item The performance cost losing the short-circuiting feature of C,C++ with boolean operators is not too high.
\end{itemize}
\item The reason for this is that multiple branches introduced by c++'s short-circuiting compilation of boolean expressions are now reduced to a single branch, which benefits from branch prediction.
\end{itemize}
\section{Branchless Computing}
\begin{itemize}
\item It refers to rewriting our code to use no branches, or at least much fewer of them.
\item It seeks to eliminate the need for branch predictors and pipeline flushes, and holding write buffers with potential actions.
\item Note that manual unrolling, such as combining
the statements for two iterations if the total number is known to be even is NOT likely to improve performance, since:
\begin{itemize}
    \item If N is large, the branch is predicted almost perfectly.
    \item The compiler may have done the unrolling anyway as an optimization.
    \item Or, a vectorizing compiler willuse SSE or AVX instructions to implement this loop.
\end{itemize}
\item Manual unrolling doesn't help because usually, the compiler would have done it anyways.
\end{itemize}
\subsection{Branchless Selection}
\begin{itemize}
\item This involves using booleans to index arrays. Ex:
\begin{lstlisting}
unsigned long a1 = 0, a2 = 0;
for (size_t i = 0; i < N; ++i) {
        if (b1[i]) {
            a1 += p1[i];
        } else {
            a2 += p1[i];
        }
}
//is replaced by:
unsigned long a1 = 0, a2 = 0;
unsigned long* a[2] = { &a2, &a1 };
for (size_t i = 0; i < N; ++i) {
    a[b1[i]] += p1[i];
}
\end{lstlisting}
\item It should be ensured that boolean values b1[i] only hold zero or one.
\item As such the second code can be pipelined, the branchless version delivers significant performance improvement.
\item Some compilers implment \texttt{?:} using a lookup array where possible, instead of a conditional branch. Thus replacing the if block by the ternary operator would also optimize the code.  
\end{itemize}
\subsection{More Examples}
\begin{itemize}
\item There is a limit to how many extra computations can be in the branchless version of a code while maintaining a performance improvement from the branched version. Additionally, if the branch prediction is effective, it will likely outperform the branchless version.
\item Also, if an instruction is optimized by a compiler using vector instructions like SSE or AVX, it may not benefit from branchless transformations.
\item Lastly, if function call branches are transformed like so to use function pointers, it hinders inlining, which is a major optimization, one that is never justified to be given up to get rid of a branch.
\end{itemize}
\chapter{Computer Architecture (Biswa's Playlist)}
\section{Introduction}
\begin{itemize}
\item MIPS is a simple yet expressive ISA.
\item Its basic principles are similar to Arm.
\item Still in use today: embedded devices, routers, modems.
\item ISA is an abstraction:
\begin{itemize}
    \item It is an interface between hardware and software.
    \item Hides complexity of execution from the software through a set of simple instructions.
\end{itemize}
\item Operands for an instruction can be in registers or in the memory.
\item Registers are closer to the processor, and take 1 unit of time to read. The memory  (DRAM) is much costlier (~100) to access than registers.
\item The ISA conveys to the programmer what the processor can/cannot do.
\item An instruction from the ISA conveys to the processor what it needs to do.
\item Up ahead, we restrict the discussion to the MIPS ISA.
\end{itemize}
\section{MIPS Instructions}
\subsection{Processor (aka Core)}
\begin{itemize}
\item It contains:
\begin{itemize}
\item Registers (~dozen), considered 32bits here.
\item ALU
\end{itemize}
\item There exist an address bus (carries addresses from processor to memory) and a data bus (carries the data from memory to processor) connecting the processor to the memory.
\end{itemize}
\subsection{Instructions}
\begin{itemize}
\item Instructions allow for operations on 16bits constants at most.
\item Most instructions have two sources and one destination. (op dst src1 src2)
\item Where src1, src2 and dst refer to registers.
\item Complex instructions are broken down into simpler registers with some registers holding intermediate results (temporary variables, registers called temporary register).
\item Special treatment of zero:
\begin{itemize}
\item \$0 is a special register that contains 0.
\item \texttt{a = b} is encoded to add a,b,\$0
\end{itemize}
\item Pseudo instruction: not really a separate instruction in the ISA, but it is included for the programmer's conenience. Ex: move a,b actually translates to the same binary as add a,b,\$0.
\item Bitwise operations in MIPS: sll, srl (shift left,right), and, or, nor, andi, ori. (i $\implies$ immediate, one of the operands is a constant.) (Not is nor with \$0).
\item Note:
\begin{itemize}
    \item lui: loads the supplied 16bit constant into upper 16bits of the register.
    \item ori, addi: operate on lower bits of a register with the 16bit supplied constant.
\end{itemize}
\end{itemize}
\subsection{Memory Instructions}
\begin{itemize}
\item Von Newmann came up with the idea of instructions being stored in memory as well as the data. Hence the program is stored in memory as the binary.
\item A word = 4B = 32bits.
\item Say our DRAM has 4GB capacity, and is a collection of words.
\item Program counter:
\begin{itemize}
\item Special register that stores the address of the instruction.
\item 32bit processor $\implies$ the addresses are of 32bits.
\item So, the processor fetches PC, PC+4,...sequentially.
\end{itemize}
\item The access time for registers increases if there are more registers, thus, we keep a limited number of registers.
\item Load and store:
\begin{itemize}
    \item lw \$t0, 1(\$a0) is \$t0 = Memory[\$a0+1];
    \item sw \$t0, 3(\$a0) is Memory[\$a0+3] = \$t0;
\end{itemize}
\item Decision making instructions:
\begin{enumerate}
    \item beq \$t0,\$t1,L1: branch equals to; Goto L1 if t0 and t1 are equal.
    \item bne \$t0,\$t1,L1: branch not equals to; Goto L1 if t0 and t1 are not equal.
\end{enumerate}
\item The above two are used with slt: set on less than.
\item slt \$t3,\$t1,\$t2: set t3 1 if t1 < t2, 0 otherwise.
\item slti has a constant in place of t2 or t1.
\begin{lstlisting}
while(arr[i]==k){
    i+=1;
}
// is converted to:
// $ ecluded because my wrists hurt.
/*
//start with s3=i, s5=k, s6=&arr[0]
loop:
sll t1,t3,2 //i*4
add t1,t1,s6// t1 += arr
lw t0,0(t1)//t0 = *t1
bne t0,s5,exit//
addi s3,s3,1//i = i+1
j loop

exit://do nothing

*/
\end{lstlisting}
\item The j instruction loads an immediate into PC. It can be specified as an offset or the label, and the assembler conerts the label into an offset.
\item Skipped lectures 4,5 and 6.
\end{itemize}
\section{Instruction Decoding}
\section{Pipeline Hazards}
\begin{itemize}
\item A hazard is anything that prevents an instruction to move ahead in the pipeline.
\item Three types:
\begin{itemize}
\item Structural hazard
\item Data hazard
\item Control hazard
\end{itemize}
\item Structural hazard happens due to two instructions trying to access a resource at the same time.
\item They are highly infrequent.
\item Registers are edge-triggered (edge of the clock graph), which means we can read/write the same register file in same clock cycle. 
\item Yet, if there is only one port to the register file for data, we get structural hazards.
\item Similarly, if there's a single memory (instruction+data), structural hazard.
\item Data Hazards refer to data dependency between instructions. The later instructions wait for previous ones to finish.
\begin{itemize}
\item True dependency may lead to RAW hazard. I2 reads something written to in I1. I2 has to wait for WB stage of I1 .
\item Anti dependence may lead to WAR hazard. I1 reads something written to in I2. We don't I1 to have the latest value. Read must happen before write.
\item Output dependence may lead to WAW hazard. I1 and I3 want to write with I2 in between reading. Same as WAR hazard. (It's not possible in vanilla 5-stage pipeline).
\end{itemize}
\item Each dependence may cause a hazard based on how far apart the instructions are.
\item Control hazards: arize from instructions that change the PC. Which branch do we continue fetching into the pipeline?
\item On encountering a hazard
\begin{itemize}
\item instructions cannot move forward and must wait for the hazard to get resolved.
\item The pipeline must stall.
\item Pass a bubble into the pipeline, using a control signal that keeps the PC from being updated in the micro-arch or using nops inserted by the compiler. Ex: sl \$t0 \$0
\item A bubble is equivalent to a stage of the following instruction being repeated: IF2 ID2 ID2 ID2 IE2 IM2 IWB2.
\end{itemize}
\item Similarly, with control hazards, replace fetched instruction by nop.
\item We need data hazard detectors, ideally at ID stage: (ID/EX and EX/MEM) refer to latches.
\begin{itemize}
    \item EX/MEM.RegRd = ID/EX.RegRs
    \item EX/MEM.RegRd = ID/EX.RegRt
    \item MEM/WB.RegRd = ID/EX.RegRs
    \item MEM/WB.RegRd = ID/EX.RegRt
\end{itemize}
\item Second solution to hazards: bypassing aka forwarding.
\item Route data to dependent instructions asap.
\item Output to be written to register is available after EX or after MEM stage; we shouldn't wait for WB stage to happen.
\item We take forwarding paths from these stages into the ALU input (where the newly written data is to be read). (Use the multiplexers and control signals in case of a hazard to decide which input to use).
\item Bypassing doesn't eliminate control hazards.
\item Bypassing doesn't eliminate all needs for stalls. Ex:
\begin{lstlisting}
lw r1,0(r4);
sub r3,r1,r2;//still have to stall for one cycle.
\end{lstlisting}
\item For control hazards, branch condition is checked in the execute stage.
\item Delayed branch: define branch to take place after a following instruction.
\item Introduce instructions independent of branching between the branch instructions and the branch target if taken.
\item Choose instructions to put in the branch delay slot from:
\begin{itemize}
    \item From before the branch. (Best choice)
    \item From branch target.
    \item From fall through.
\end{itemize}
\item Stalls impact the CPI and thus the program speed.
\end{itemize}
\subsection{Branch Prediction}
\begin{itemize}
\item BP needs to happen in the fetch stage. 
\mygraphic{rsrc/branchpred.png}
\item Static Direction Predition: 
\begin{itemize}
    \item Always not-taken. Simple to implement; doesn't need BTB, doesn't need direction prediction. Accuracy < 40\%
    \item Always taken: No direction prediction, but needs BTB. Accuracy: >60\%.
\end{itemize}
\item Dynamic Predictors:
\begin{itemize}
    \item Last Time predictor: 
    \mygraphic{rsrc/statebp.png}
\end{itemize}
\end{itemize}
\subsection{Implementation}
\begin{itemize}
\item K bits of branch instruction address used to index in a table of $2^K$ entries, 1 bit per entry.
\item Always mispredicts first and last iteration. Accuracy = (N-2)/N. (Or init to 1, and (N-1)/N)
\item Alternating taken/not-taken gives 0\% accuracy.
\item Mispredictions lead to stalls.
\item Alternatively, use 2bit predictors, that switch the prediction after two continuous mispredictions.
\item Implemented using saturating counter.
\item Bimodal predictors are good for biased branches (always taken/not taken).
\item Suppose next fetch address after a branch instruction is not determined after N cycles in a pipelined processor.
\item N cycles = minimum branch resolution latency.
\item How do we keep the pipeline full even before branch prediction?
\item Increase in bp accuracy even at 0.5\% level is useful.
\end{itemize}
\subsection{Local and Global History}
\begin{itemize}
\item Local behaviour: prediction(A) given last k instances of branch A.
\item Global behaviour: prediction(Z) given the outcomes of all previous branches A,B,C,..Z?
\item Two level branch predictors.
\item First level: global branch history register; the direction of the last N branches. (Branch history table) Entry used to index into pattern history table.
\item Second level: Table of saturating counter for each history entry. (Pattern history table).
\item PHT is 2 bit predictor usually.
\item Set of branches: one register for correlated ones.
\item suspended here.
\end{itemize}
\section{Memory Hierarchy}
\subsection{Hierarchy}
\begin{itemize}
\item DRAM is capacitor based. Data stored as charge. DRAM is denser.
\item Capacitor needs to be refreshed periodically to maintain the charge despite leaking.
\item DRAMs are accessed as a table: row index and column index.
\item SRAMs take 6T to 8T to store one bit.
\item Low density, doesn't need refreshing, faster.
\item Bigger is slower.
\item Faster is more expensive.
\item Memory hierarchy.
\begin{itemize}
\item We want both fast and large memory.
\item Needs a hierarchy.
\end{itemize}
\item CPU talks to small, fast memory (RF, SRAM): talks to big slow memory (DRAM).
\item SRAM are is a part of the processor (on chip)
\item DRAM is not on chip. 
\end{itemize}
\subsection{Caches}
\begin{itemize}
\item Costly DRAM accesses (~200 cycles) without caches.
\item Bandwidth problems can be solved with more buses, but latency problems are bounded by the speed of light.
\item DRAM controller on chip, mediates data transfer.
\item Memory wall: speed difference between processor and memory access.
\item Common case idea: reduce time of execution of common case execution path.
\item Locality:
\begin{itemize}
\item Temporal: same address accessed repeatedly over a period of time.
\item Spatial: nearby addresses accessed closely in time.
\end{itemize}
\item Caches exploit these localities speculatively.
\item Multiple level of caches: cache hierarchy.
\begin{itemize}
\item L1:closest to cpu, fastest, smallest. Latency and bandwidth in mind (multiple ports)
\item L2: (Latency)
\item L3:furthest from cpu, slowest, largest. (Capacity)
\end{itemize}
\end{itemize}
\subsection{Cache Mapping Techniques}

\chapter{Javascript Notes from \href{https://www.interviewbit.com/javascript-interview-questions/}{InterviewBit}}
\section{Data Types, typeof and instanceof from \href{https://www.w3schools.com/js/js_typeof.asp}{W3Schools}}
\begin{itemize}
\item There are five different data types to hold values:
\begin{enumerate}
    \item string
    \item number
    \item boolean
    \item object
    \item function
\end{enumerate}
\item There are 6 types of objects:
\begin{enumerate}
\item Object
\item Date
\item Array
\item String
\item Number
\item Boolean
\end{enumerate}
\item There are 2 data types that cannot contain values:
\begin{enumerate}
\item null (typeof returns object though)
\item undefined
\end{enumerate}
\item The \texttt{typeof} operator returns the data type of a variable.
\begin{lstlisting}
// Returns "string"
typeof "John"
//Returns number:
typeof 3.14
typeof NaN
// Returns "boolean:"
typeof false
// Returns "object:"
typeof [1,2,3,4]              
typeof {name:'John', age:34}  
typeof new Date()
typeof null
// Returns "function:"
typeof function () {}         
//Returns "undefined:"
typeof myCar //variable not yet defined.
typeof undefined//keyword undefined.
\end{lstlisting}
\item \texttt{typeof} can't be used to determine if an object is an array or a date.
\item Primitive Data: single simple data value with no additional properties or methods. typeof returns one of "string", "number", "boolean" or "undefined".
\item Non-primitive, aka Reference Data: typeof returns "function" or "object".
\item To check if an object is an array (and similarly for Date):
\begin{lstlisting}
function isArray(myArray) {
  return myArray.constructor === Array;
}
\end{lstlisting}
\item Empty Values: need not be undefined. Ex. an empty sring has both a legal value and a type.
\item undefined and null are equal in value but different in type.
\begin{lstlisting}
null === undefined // false
null == undefined // true
\end{lstlisting}
\item \texttt{instanceof} operator returns true if an object is an instance of the specified object.
\item Note some peculiarites below:
\begin{lstlisting}
const cars = ['a','b','c'];
cars instanceof Array; //true
cars instanceof Object; //true
cars instanceof string[];
/*Syntax error, rhs of instanceof is not an Object*/
let a = cars[0];
a instanceof string;
/*Syntax error, rhs of instanceof is not an Object*/
a instanceof String;//false
//a has primitive type string, not String.
a = Object(cars[0])
a instanceof Object;//true
a instanceof String;//true

//Additionally
a = {c:'b'};
typeof a; // returns object.
a instanceof Object;//return true 
\end{lstlisting}
\item \texttt{void} operator takes a piece of code and ensures voids the return value (to return undefined)
\end{itemize}
\section{Notes from Questions}
\begin{itemize}
\item Initially used for building dynamic web pages.
\item Hoisting: all variable and function declarations are moved to the top of the code by the interpreter. This means order of declaration is irrelevant.
\item \texttt{"user strict"}, with quotes at the start of a file turns off hoisting.
\item \texttt{==} compares values of two variables while \texttt{===} compares values and types.
\item var vs let:
\begin{enumerate}
    \item var variables are hoisted, let variables are not.
    \item var variables have function scope, let variables have block scope.
    \item let variables are closer to other programming languages in terms of the first two points.
\end{enumerate}
\item Implicit type coercion: when data types of two operands don't match, implicit type coercion takes place.
\begin{enumerate}
    \item String coercion: 3+"3" = "33";
    \item Number coercion: 3-"3" = 0, because there's no string operator corresponding to -.
    \item Boolean coercion: takes place when using logical operators, ternary operators, if statments and loop checks. Involves understanding:
    \begin{enumerate}
        \item Falsy values: values coerced to false. The exhaustive set is false, 0, 0n, -0, "", null, undefined, NaN.
        \item Truthy values: values not coerced to false.
    \end{enumerate}
    \item Equality coercion: happens with == operator. Both variales are converted to the same type and their values are compared. No coercion happens with ===.
\end{enumerate}
\item Logical operators ($||$ and \&\&) return one of the operands, not true or false, based on which is truthy and falsy, following lazy evaluation.
\item It's a dynamically typed language, so variable types are checked during runtime and can change over runtime. Statically typed languages have type checks during compile time.
\item \texttt{NaN} indicates a value that is not a legal number.
\begin{itemize}
    \item typeof NaN returns number.
    \item use isNaN(x) to check if x is NaN.
    \item only non-numeric and non-empty strings (empty$\implies$0), undefined and NaN return true for isNaN()
    \item Even '131n' return true for isNaN.
\end{itemize}
\item Mutability: the property of a variable to be modified without using the assignment operator.
\item Primitive data types are immutable.
\item Non-primitive data types, aka reference data types, are mutable. These include Objects and Functions.
\item All \texttt{objects} are on the heaps, and the variables that hold them are actually pointers that reside on the stack.
\begin{lstlisting}
const staff2 = staff;//where staff is an Object.
//staff2 as a pointer holds the same address as staff, so:
staff2.name = '';//also modifies staff.
\end{lstlisting}
\item To copy an object, use
\begin{lstlisting}
const staff2 = Object.assign({},staff);//cast if necessary, like Array.from...
//or
const staff2 = {...staff};
\end{lstlisting}
\item Note that both ... and Object.assign only create shallow copies; the reference properties of the objects inside are still copied by reference:
\begin{lstlisting}
let b = [1,2,3];
let d = {j:b};
d.j.push(2);//b is modified too.
let c = {...d};
c.j.push(2);//b is modified too.
let e = Object.assign({},d);
e.j.push(2);//b is modified too.
\end{lstlisting}
\begin{lstlisting}
let b = [1,2,3];
let d = {j:b};
\end{lstlisting}
\item References in javascript:
\begin{itemize}
\item Primitive data types are passed by value, non-primitive data types are passed by reference.
\end{itemize}
\item Objects can be made immutable by using:
\begin{enumerate}
\item \texttt{Object.preventExtensions(staff)}: disallows adding new properties. 
\begin{itemize}
    \item Properties can still be deleted or modified.
    \item New property definitions using '.' and assignment fail silently.
    \item New property definitions using \texttt{Object.defineProperty} throw a TypeError.
\end{itemize}
\item \texttt{Object.seal(staff)}: Only permits modifying existing properties. 
\begin{itemize}
\item Deletion and addition of properties is disallowed.
\item \texttt{Object.isSealed(staff)} used to check if an object is sealed.
\item '.' operations fail silently
\item \texttt{defineProperty} for new properties throw a TypeError.
\item Deletions fail silently.
\end{itemize}
\item \texttt{Object.freeze(staff)} disallows all modifications, including deletion and insertion of properties.
\begin{itemize}
\item Check using \texttt{Object.isFrozen(staff)}
\item Same rules as errors/silent failing as above.
\item It's not a recursize freeze. Properties of reference properties of a frozen object can still be modified. A recursive deep-freeze function may be written to avoid this fall.
\end{itemize}
\end{enumerate}
\item Variables declared using \texttt{const} may be mutable or immutable; but they can't be assigned to later.
\end{itemize}
\section{Anti-patterns}
Summarized from blogs \href{https://dev.to/medhatdawoud/javascript-anti-patterns-2nia}{one} and \href{https://javascript.plainenglish.io/javascript-anti-patterns-we-do-every-day-3d0086e2910}{two.}
\begin{itemize}
\item Array operations:
    \begin{itemize}
        \item Use \texttt{map} only to create a new array from a given array.
        \item Use \texttt{for-loops} only for sequential element processing.
        \item Use \texttt{forEach} only for independent element processing.
    \end{itemize}
\item \texttt{let, const and var}:
    \begin{itemize}
        \item \texttt{let} and \texttt{const} variables are blockscoped, while \texttt{var} are not. Prefer using block-scoped variables as they reduce the chances of bugs related to shadowing.
        \item For all constant variables, use \texttt{const};
    \end{itemize}
\item Promises: offer several code management advantages, such as:
\begin{itemize}
    \item No more callback pyramid of doom:
    \begin{lstlisting}
        doSomething(function (result) {
            doSomethingElse(result, function (newResult) {
                doThirdThing(newResult, function (finalResult) {
                console.log(`Got the final result: ${finalResult}`);
                }, failureCallback);
            }, failureCallback);
        }, failureCallback);
        //replaced by:
        doSomething()
        .then(function (result) {
          return doSomethingElse(result);
        })
        .then(function (newResult) {
          return doThirdThing(newResult);
        })
        .then(function (finalResult) {
          console.log(`Got the final result: ${finalResult}`);
        })
        .catch(failureCallback);
    \end{lstlisting}
    \item Note the importance of the \texttt{return} statments in the chain. A floating promise in such a chain is one
    without a return statement, whose output is thus not passed to the following then clauses.
    \item Floating promises can lead to race conditions. Ex:
    \begin{lstlisting}
        const listOfIngredients = [];
        doSomething()
        .then((url) => {
            // Missing `return` keyword in front of fetch(url).
            fetch(url)
            .then((res) => res.json())
            .then((data) => {
                listOfIngredients.push(data);
            });
        })
        .then(() => {
            console.log(listOfIngredients);
            // listOfIngredients will always be [], because the fetch request hasn't completed yet.
        });
    \end{lstlisting}
    \item Using async/await allows for writing code with promises that resembles synchronous code closely.
    \item \texttt{.then(callback)} attached after \texttt{.catch} is both with and without execution of the catch statement.
\end{itemize}
\item Promise composition:
\begin{itemize}
    \item \texttt{Promise.all([p1,p2,1,abc])} await all promises (non-sequentially) and return array with values returned by them. It rejects with the error from the first promise being rejected.
    \item For sequential promise resolutions:
    \begin{lstlisting}
        [func1, func2, func3]
        .reduce((p, f) => p.then(f), Promise.resolve())
        .then((result3) => {
            /* use result3 */
        });
        //Think of Promise.resolve as a blank promise that returns undefined immediately.
        //Promise.resolve("value").then((v)=>{console.log(v);})//"value". 
    \end{lstlisting}
    Or using async/await:
    \begin{lstlisting}
        let result;
        for (const f of [func1, func2, func3]) {
        result = await f(result);
        }
        /* use last result (i.e. result3) */

    \end{lstlisting}
    \item \texttt{Promise.allSettled([p1,p2,p3])} works like \texttt{Promise.all}, except it never rejects. It waits for
    all promises to be resolved or rejected and returns an array of status-value objects:
    \begin{lstlisting}
        Promise.allSettled([
        Promise.resolve(33),
        new Promise((resolve) => setTimeout(() => resolve(66), 0)),
        99,
        Promise.reject(new Error("an error")),
        ]).then((values) => console.log(values));
        /* [
           { status: 'fulfilled', value: 33 },
           { status: 'fulfilled', value: 66 },
           { status: 'fulfilled', value: 99 },
           { status: 'rejected', reason: Error: an error }
         ] */
    \end{lstlisting}
    \item \texttt{Promise.any([p1,p2,p3])} resolves with the value from the first (quickest) to be resolved. If an empty array is passed or if all the promises are rejected, it rejects with an array containing reject-values for each promise.
    \item \texttt{Promise.race([p1,p2,p3])} resolves when the first (quickest) promise rejects or resolves. 
\end{itemize}
\item Promise error handling: (References \href{https://stackoverflow.com/questions/42013104/placement-of-catch-before-and-after-then}{one} and \href{https://stackoverflow.com/questions/24662289/when-is-thensuccess-fail-considered-an-antipattern-for-promises}{two})
\begin{lstlisting}
P.then((v)=>{return transform(v);}).catch((err)=>{handle(err);});
//handle will be called for errors from P and for errors in transform(v).
P.then((v)=>{return transform(v);},(err)=>{handle(err)}).catch((err)=>{handleTransformError(err)});
//handle will be called only for errors from P.
\end{lstlisting}
\item Nesting: convert nested if-checks to flattened if (invalid), return statements.
\item Naming convention: adjectives prefixing nouns, camelCase.
\item When definining useful constants (that can be written in terms of other constants),
\begin{itemize}
    \item Use uppercase snake\_case for names.
    \item Have the definition convey be meaningful in terms of other constants.
\end{itemize}
\item DOM manipulation calls (\texttt{document.appendChild}) should not be done in loops.
\item Avoid creating extra objects in memory during accumulation operations:
\begin{lstlisting}
    const users = [
  { name: "Medhat", admin: true },
  { name: "Adam", admin: false },
  { name: "Karma", admin: true },
]

// spread operator is creating a new Obj
users.reduce(
  (acc, item) => ({
    ...acc,
    [item.name]: item.admin,
  }),
  {}
)
//better: 
users.reduce((acc, item) => {
  acc[item.name] = item.admin
  return acc
}, {})

\end{lstlisting}
\end{itemize}
\chapter{Python}
\section{Misc.}
\begin{itemize}
\item The else block at the end of a for loop is executed only if the for loop was exited without using a break statement.
\item Same rule applies to an else block at the end of a while loop.
\item The try-except-else-finally block works such that:
\begin{itemize}
\item Try block is always executed.
\item Except block catches particular errors if thrown and hence executes only if errors arise.
\item Else block executes if NO error was encountered.
\item Finally block is always executed.
\end{itemize}
\item TODO list:
\begin{itemize}
\item try-catch-finally better reads.
\item with block.
\item functions with args, kwargs.
\item sys.argv etc.
\item match case statement.
\item python pass by reference or value.
\end{itemize}
\end{itemize}
\chapter{Solid Principles \href{https://stackoverflow.blog/2021/11/01/why-solid-principles-are-still-the-foundation-for-modern-software-architecture/}{from Stackoverflow}}
\begin{itemize}
\item Solid is a set of principles distilled from the writings of Robert C. Martin in the early 2000s. It is a way to think specifically about:
\begin{itemize}
    \item the quality of OOP
    \item how the code should be split up
    \item which parts should be internal/exposed
    \item how code should use other code.
\end{itemize}
\item Some major changes in the industry after the development of the solid principles are:
\begin{itemize}
    \item Dynamically-typed languages
    \item Non-OOP paradigms: functional programming.
    \item Open source software has proliferated.
    \item Microservices, SAAS: services that talk to other services.
\end{itemize}
\item These changes imply that many things SOLID really cared about: classes, interfaces, data hiding, polymorphism, are not what programmers deal with every day.
\item What hasn't changed:
\begin{itemize}
\item Code is written and modified by people over and over again. There will always be a need for well-documented code and APIs.
\item Code is organized into modules. Each language provides some way of organizing code into distinct, bounded units. Thus, there will always be a need to decide how best to group code together.
\item Code can be internal or external: some code is written for use within the team, while some is written for use by consumers or other teams. There is a need to decide what is visible and hidden.
\end{itemize}
\item Up we discuss SOLID principles as applicable to modern programming.
\end{itemize}
\section{Single Responsibility Principle}
\begin{itemize}
\item Each class (module) should do one job, and do it well. Our code shouldn't mix multiple roles or purposes together: high cohesion.
\item We shouldn't have to modify the code for that class unless there are changes to its specific function, or the interface it provides, or the interfaces it uses.
\end{itemize}
\section{Open-closed Principle}
\begin{itemize}
\item Software entities (classes) should be open for (allow) extension (via inheritance), but closed for (disallow) modification. (Once the code is sent into production).
\item This limits the dependency on the author of the class. Additionally, multiple changes could result in the class incorporating too many concerns.
\item It also protects the code from unskilled hands.
\item We should be able to use and add to a module (via inheritance), without rewriting it.
\item In the FP world, this involves defining explicit hook points in the base method to allow for before and after modifications, as well as a way to override the base behaviour. 
\begin{lstlisting}
// library code

const saveRecord = (record, save, beforeSave, afterSave) => {
    const defaultSave = (record) => {
    // default save functionality
    }
    
    if (beforeSave) beforeSave(record);
    if (save) {
    save(record);
    }
    else {
    defaultSave(record);
    }
    if (afterSave) afterSave(record);
}
    
// calling code
    
const customSave = (record) => { ... }
saveRecord(myRecord, customSave);
\end{lstlisting}
\end{itemize}
\section{Liskov Substitution Principle}
\begin{itemize}
\item Objects of a subclass should be allowed to be substituted in methods involving objects of the superclass, without altering any desirable properties of the program (breaking anything).
\item This allows for/encourages polymorphism in OOPS.
\item In FP, it encourages function parameters, as passed to filter and map in javascript.
\end{itemize}
\section{Interface Segregation Principle}
\begin{itemize}
\item Many client-specific interfaces are better than one general-purpose interface.
\item Don't show your client (this could be another piece of code using an interface/type), more than what they need to see.
\begin{lstlisting}
interface PrintRequestModifier {
   public void createRequest();
   public void deleteRequest();
}
 
interface PrintRequestWorker {
   public void workOnRequest()
}
 
class PrintRequest implements PrintRequestModifier, PrintRequestWorker {
   public void createRequest() {}
   public void deleteRequest() {}
   public void workOnRequest() {}
}
\end{lstlisting}
\item This decreases coupling and ensures that a client doesn't need to know about, or depend on, features that it has no intention of using.
\end{itemize}
\section{Dependency Inversion Principle}
\begin{itemize}
\item Depend on abstractions, not concretions.
\item In OO, this means that clients should depend on interfaces rather than concrete classes as much as possible; this minimizes the surface area of the code that the client depends on.
\end{itemize}
\chapter{Aptitude Test Pointers}
\input{parts/aptitude}
\end{document}